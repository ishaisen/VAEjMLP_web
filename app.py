# =====================================================
# VAEjMLP latent-SHAP + ç¨³å®šæ€§ + SHAPå¯è§†åŒ– + CoxéªŒè¯ (Streamlit)
# =====================================================
import streamlit as st
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score

import shap
import matplotlib.pyplot as plt


# =====================================================
# Utils
# =====================================================
def set_seed(seed: int):
    import random

    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)

    # è®©ç»“æœæ›´å¯å¤ç°ï¼ˆå¯èƒ½ç•¥æ…¢ï¼‰
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


@st.cache_data(show_spinner=False)
def read_csv_cached(uploaded_file) -> pd.DataFrame:
    return pd.read_csv(uploaded_file)


# =====================================================
# æ¨¡å‹å®šä¹‰ï¼ˆä¸ä½ åŸå§‹ä¸€è‡´ï¼‰
# =====================================================
class VAE(nn.Module):
    def __init__(self, input_dim, latent_dim):
        super().__init__()
        self.fc1 = nn.Linear(input_dim, 1024)
        self.fc2 = nn.Linear(1024, 512)
        self.fc3 = nn.Linear(512, latent_dim * 2)

    def encode(self, x):
        h1 = F.relu(self.fc1(x))
        h2 = F.relu(self.fc2(h1))
        h3 = self.fc3(h2)
        mean, log_var = torch.chunk(h3, 2, dim=-1)
        return mean, log_var

    def reparameterize(self, mean, log_var):
        std = torch.exp(0.5 * log_var)
        eps = torch.randn_like(std)
        return mean + eps * std

    def forward(self, x):
        mean, log_var = self.encode(x)
        z = self.reparameterize(mean, log_var)
        return z, mean, log_var


class MLP(nn.Module):
    def __init__(self, latent_dim):
        super().__init__()
        self.fc1 = nn.Linear(latent_dim, 128)
        self.fc2 = nn.Linear(128, 64)
        self.fc3 = nn.Linear(64, 1)

    def forward(self, z):
        z = F.relu(self.fc1(z))
        z = F.relu(self.fc2(z))
        return torch.sigmoid(self.fc3(z))


# =====================================================
# Streamlit é¡µé¢
# =====================================================
st.set_page_config(page_title="VAEjMLP latent-SHAP", layout="wide")
st.title("ğŸ§¬ VAEjMLP + latent SHAP ç”Ÿç‰©æ ‡å¿—ç‰©åˆ†æï¼ˆå®Œæ•´æ•´åˆç‰ˆï¼‰")

with st.sidebar:
    st.header("å‚æ•°è®¾ç½®")

    latent_dim = st.number_input("latent_dim", min_value=4, max_value=1024, value=128, step=4)
    n_epochs = st.number_input("è®­ç»ƒè½®æ•° epochs", min_value=10, max_value=2000, value=100, step=10)
    lr = st.number_input("å­¦ä¹ ç‡ lr", min_value=1e-5, max_value=1e-1, value=1e-3, step=1e-4, format="%.5f")

    ce_weight = st.number_input("CE æƒé‡ï¼ˆloss = KL + ce_weight*CEï¼‰", min_value=0.0, max_value=10.0, value=0.001, step=0.001)

    test_size = st.slider("test_size", 0.05, 0.5, 0.2, 0.05)

    st.divider()
    st.subheader("ç¨³å®šæ€§ (multi-run)")

    n_runs = st.slider("é‡å¤è¿è¡Œæ¬¡æ•° n_runs", 1, 50, 10)
    top_k = st.slider("TopK é¢‘ç‡ç»Ÿè®¡", 5, 300, 20)
    seed_base = st.number_input("seed_base", value=42, step=1)

    st.divider()
    st.subheader("SHAP è®¡ç®—")

    background_n = st.slider("background æ ·æœ¬æ•°", 10, 200, 50)
    shap_nsamples = st.slider("KernelExplainer nsamples", 50, 500, 100, 50)

    st.caption("æç¤ºï¼šKernelExplainer å¯èƒ½è¾ƒæ…¢ï¼›n_runs å¤§æ—¶å»ºè®®é™ä½ nsamples æˆ– backgroundã€‚")

    st.divider()
    st.subheader("Cox ç”Ÿå­˜éªŒè¯ï¼ˆå¯é€‰ï¼‰")

    use_survival = st.checkbox("å¯ç”¨ Cox ç”Ÿå­˜éªŒè¯", value=False)
    freq_thr = st.slider("ç¨³å®šåŸºå› ç­›é€‰ï¼šTopK_Freq â‰¥", 0.0, 1.0, 0.6, 0.05)
    cv_thr = st.number_input("ç¨³å®šåŸºå› ç­›é€‰ï¼šCV â‰¤", min_value=0.0, max_value=50.0, value=1.0, step=0.1)
    max_surv_genes = st.slider("æœ€å¤šç”¨äº Cox çš„åŸºå› æ•°", 5, 300, 50)
    cox_penalizer = st.number_input("Cox L2 penalizer", min_value=0.0, max_value=10.0, value=0.1, step=0.1)

st.divider()

rna_file = st.file_uploader("ä¸Šä¼  RNA-seq è¡¨è¾¾çŸ©é˜µï¼ˆgenes Ã— samplesï¼‰ï¼ŒCSVï¼Œindex=Geneï¼Œcolumns=Sample", type="csv")
label_file = st.file_uploader("ä¸Šä¼  Label æ–‡ä»¶ï¼ˆSample, Labelï¼‰ï¼ŒCSV", type="csv")
surv_file = st.file_uploader("ï¼ˆå¯é€‰ï¼‰ä¸Šä¼  ç”Ÿå­˜æ•°æ®ï¼ˆSample, Time, Eventï¼‰ï¼ŒCSV", type="csv") if use_survival else None

run_button = st.button("ğŸš€ è¿è¡Œæ¨¡å‹")

# =====================================================
# ä¸»æµç¨‹
# =====================================================
if run_button:
    if (rna_file is None) or (label_file is None):
        st.error("è¯·å…ˆä¸Šä¼  RNA è¡¨è¾¾çŸ©é˜µå’Œ Label æ–‡ä»¶ã€‚")
        st.stop()

    with st.spinner("è¯»å–æ•°æ®ä¸­..."):
        rna = read_csv_cached(rna_file)
        labels = read_csv_cached(label_file)

    # ---- æ•°æ®æ ¡éªŒä¸å¯¹é½ ----
    if rna.shape[0] < 2 or rna.shape[1] < 4:
        st.error("RNA çŸ©é˜µç»´åº¦çœ‹èµ·æ¥ä¸å¯¹ï¼šéœ€è¦ genesÃ—samples ä¸”æ ·æœ¬æ•°è‡³å°‘ 4ã€‚")
        st.stop()

    if "Sample" not in labels.columns or "Label" not in labels.columns:
        st.error("Label æ–‡ä»¶å¿…é¡»åŒ…å«åˆ—ï¼šSample, Label")
        st.stop()

    # é»˜è®¤ï¼šrna ç¬¬ä¸€åˆ—ä¸º gene indexï¼ˆå¦‚æœç”¨æˆ·æ²¡è®¾ç½® index_colï¼‰
    if "Unnamed: 0" in rna.columns:
        rna = rna.rename(columns={"Unnamed: 0": "Gene"}).set_index("Gene")

    # å¯¹é½æ ·æœ¬
    samples_rna = list(rna.columns)
    samples_lab = labels["Sample"].astype(str).tolist()
    rna.columns = rna.columns.astype(str)
    labels["Sample"] = labels["Sample"].astype(str)

    if set(samples_rna) != set(samples_lab):
        st.warning("RNA æ ·æœ¬ä¸ Label æ ·æœ¬é›†åˆä¸å®Œå…¨ä¸€è‡´ï¼Œå°†å–äº¤é›†å¯¹é½ã€‚")
        common = sorted(list(set(samples_rna).intersection(set(samples_lab))))
        if len(common) < 4:
            st.error("å¯¹é½åå…±åŒæ ·æœ¬æ•°å¤ªå°‘ï¼ˆ<4ï¼‰ï¼Œæ— æ³•è®­ç»ƒã€‚")
            st.stop()
        rna = rna[common]
        labels = labels.set_index("Sample").loc[common].reset_index()

    # é‡æ–°æ ¡éªŒé¡ºåºä¸€è‡´
    labels = labels.set_index("Sample").loc[rna.columns].reset_index()

    genes = rna.index.astype(str).tolist()
    y = labels["Label"].values.astype(int)

    # X: samples x genes
    X = MinMaxScaler().fit_transform(rna.T.values)

    # =====================================================
    # å¤šæ¬¡è¿è¡Œï¼šæ”¶é›† metrics / gene_importance / shap_zï¼ˆä»…æœ€åä¸€æ¬¡ç”¨äºç”»å›¾ï¼‰
    # =====================================================
    all_importances = []     # list[pd.Series] index=genes
    topk_lists = []          # list[list[str]]
    metrics_runs = []        # list[dict]
    last_shap_z = None
    last_z_test = None

    # å¦‚æœç”¨æˆ·è·‘å¾ˆå¤šæ¬¡ï¼šç»™ä¸ªè¿›åº¦æ¡
    prog = st.progress(0)
    status = st.empty()

    for run_i in range(int(n_runs)):
        seed = int(seed_base + run_i)
        set_seed(seed)

        status.write(f"Run {run_i+1}/{n_runs} | seed={seed}")

        # ---- split ----
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=float(test_size), random_state=seed, stratify=y if len(np.unique(y)) == 2 else None
        )

        X_train_t = torch.tensor(X_train, dtype=torch.float32)
        X_test_t = torch.tensor(X_test, dtype=torch.float32)
        y_train_t = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)

        # ---- æ¨¡å‹ ----
        vae = VAE(X.shape[1], int(latent_dim))
        mlp = MLP(int(latent_dim))
        optimizer = optim.Adam(list(vae.parameters()) + list(mlp.parameters()), lr=float(lr))

        # ---- è®­ç»ƒ ----
        vae.train()
        mlp.train()
        for _ in range(int(n_epochs)):
            optimizer.zero_grad()
            z, mean, log_var = vae(X_train_t)
            y_pred = mlp(z)
            kl = -0.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp())
            ce = F.binary_cross_entropy(y_pred, y_train_t, reduction="sum")
            loss = kl + float(ce_weight) * ce
            loss.backward()
            optimizer.step()

        vae.eval()
        mlp.eval()

        # ---- é¢„æµ‹ä¸æŒ‡æ ‡ ----
        with torch.no_grad():
            z_test, _, _ = vae(X_test_t)
            y_pred_test = mlp(z_test).cpu().numpy().flatten()

        # ä¸€äº›æ•°æ®é›†å¯èƒ½å¯¼è‡´ AUC æŠ¥é”™ï¼ˆæµ‹è¯•é›†ä¸­åªæœ‰ä¸€ä¸ªç±»ï¼‰
        try:
            auc = roc_auc_score(y_test, y_pred_test)
        except Exception:
            auc = np.nan

        y_hat = (y_pred_test > 0.5).astype(int)

        metrics_runs.append({
            "run": run_i,
            "seed": seed,
            "AUC": auc,
            "Accuracy": accuracy_score(y_test, y_hat),
            "Precision": precision_score(y_test, y_hat, zero_division=0),
            "Recall": recall_score(y_test, y_hat, zero_division=0),
        })

        # =====================================================
        # latent SHAPï¼ˆKernelExplainerï¼‰
        # =====================================================
        with torch.no_grad():
            z_train, _, _ = vae(X_train_t)
        z_train_np = z_train.cpu().numpy()
        z_test_np = z_test.cpu().numpy()


        def mlp_predict(z_numpy):
            z_t = torch.tensor(z_numpy, dtype=torch.float32)
            with torch.no_grad():
                out = mlp(z_t).cpu().numpy()
            return out.reshape(-1)  # <-- å…³é”®ï¼šè¿”å› 1D

        bg_n = int(min(background_n, z_train_np.shape[0]))
        background_z = shap.sample(z_train_np, bg_n)

        explainer = shap.KernelExplainer(mlp_predict, background_z)

        shap_values = explainer.shap_values(z_test_np, nsamples=int(shap_nsamples))

        # --- å…¼å®¹ä¸åŒ shap ç‰ˆæœ¬çš„è¿”å›ç±»å‹/å½¢çŠ¶ ---
        if isinstance(shap_values, list):
            shap_z = shap_values[0]
        else:
            shap_z = shap_values

        # å¦‚æœæ˜¯ (n, d, 1) â†’ squeeze æˆ (n, d)
        shap_z = np.array(shap_z)
        if shap_z.ndim == 3 and shap_z.shape[-1] == 1:
            shap_z = shap_z[:, :, 0]

        # æœ€ç»ˆä¿è¯æ˜¯äºŒç»´ (n_samples, n_features)
        if shap_z.ndim != 2:
            raise ValueError(f"Unexpected shap_z shape: {shap_z.shape}")

        # ä¿é™©æ ¡éªŒï¼Œä¾¿äºä½ å®šä½
        if shap_z.shape[0] != z_test_np.shape[0] or shap_z.shape[1] != z_test_np.shape[1]:
            raise ValueError(
                f"Shape mismatch: shap_z={shap_z.shape}, z_test={z_test_np.shape}. "
                "Check mlp_predict output shape and shap_values processing."
            )

        # =====================================================
        # latent â†’ gene æ˜ å°„ï¼ˆä¿æŒä¸ä½ åŸé€»è¾‘ä¸€è‡´ï¼‰
        # =====================================================
        W_gene_hidden = vae.fc1.weight.detach().cpu().numpy()  # (1024, n_genes)
        abs_shap_z = np.mean(np.abs(shap_z), axis=0)           # (latent_dim,)

        # è¿™é‡Œä½ çš„åŸå¼ï¼šåªå·®åœ¨ W_fc1 çš„åˆ—å¼ºå¼±ï¼Œlatent shap åªæ˜¯å…¨å±€ç¼©æ”¾
        # å…ˆä¿æŒä¸€è‡´ï¼Œä¿è¯å¤ç°ï¼›åç»­ä½ éœ€è¦æ›´è®ºæ–‡çº§æ˜ å°„æˆ‘å¯ä»¥å†å‡çº§
        gene_importance = {}
        scale = float(np.sum(abs_shap_z))
        for i, gene in enumerate(genes):
            gene_importance[gene] = float(np.mean(np.abs(W_gene_hidden[:, i])) * scale)

        imp_s = pd.Series(gene_importance).reindex(genes)
        all_importances.append(imp_s)

        topk_lists.append(imp_s.sort_values(ascending=False).head(int(top_k)).index.tolist())

        # ä¿å­˜æœ€åä¸€æ¬¡ run çš„ shap ç”¨äºç”»å›¾
        if run_i == int(n_runs) - 1:
            last_shap_z = shap_z
            last_z_test = z_test_np

        prog.progress((run_i + 1) / int(n_runs))

    status.empty()
    prog.empty()

    # =====================================================
    # æ±‡æ€»è¾“å‡ºï¼šæ¨¡å‹æ€§èƒ½ï¼ˆå¤šæ¬¡runï¼‰
    # =====================================================
    metrics_df = pd.DataFrame(metrics_runs)
    st.success("è¿è¡Œå®Œæˆï¼")

    st.subheader("ğŸ“Š æ¨¡å‹æ€§èƒ½ï¼ˆæ¯æ¬¡ runï¼‰")
    st.dataframe(metrics_df)

    st.subheader("ğŸ“Š æ¨¡å‹æ€§èƒ½æ±‡æ€»ï¼ˆå‡å€¼Â±æ ‡å‡†å·®ï¼‰")
    summary = metrics_df[["AUC", "Accuracy", "Precision", "Recall"]].agg(["mean", "std"]).T.reset_index()
    summary.columns = ["Metric", "Mean", "Std"]
    st.dataframe(summary)

    st.download_button(
        "â¬‡ ä¸‹è½½æ‰€æœ‰ run çš„æ¨¡å‹æŒ‡æ ‡",
        metrics_df.to_csv(index=False),
        "model_metrics_all_runs.csv",
    )

    # =====================================================
    # æ±‡æ€»è¾“å‡ºï¼šåŸºå› é‡è¦æ€§ç¨³å®šæ€§ï¼ˆMean / CV / Frequencyï¼‰
    # =====================================================
    imp_mat = pd.concat(all_importances, axis=1)
    imp_mat.columns = [f"run_{i}" for i in range(int(n_runs))]

    mean_imp = imp_mat.mean(axis=1)
    std_imp = imp_mat.std(axis=1)
    cv_imp = std_imp / (mean_imp.abs() + 1e-12)

    from collections import Counter
    freq_counter = Counter([g for lst in topk_lists for g in lst])
    freq = pd.Series({g: freq_counter.get(g, 0) / float(n_runs) for g in genes})

    stability_df = pd.DataFrame({
        "Gene": genes,
        "MeanImportance": mean_imp.values,
        "StdImportance": std_imp.values,
        "CV": cv_imp.values,
        f"Top{int(top_k)}_Freq": freq.values,
    }).sort_values([f"Top{int(top_k)}_Freq", "MeanImportance"], ascending=[False, False])

    st.subheader("ğŸ“Œ ç”Ÿç‰©æ ‡å¿—ç‰©ç¨³å®šæ€§ï¼ˆFrequency / CVï¼‰")
    st.dataframe(stability_df.head(50))

    st.download_button(
        "â¬‡ ä¸‹è½½ç¨³å®šæ€§ç»Ÿè®¡è¡¨",
        stability_df.to_csv(index=False),
        "biomarker_stability.csv",
    )

    # å…¼å®¹ä½ åŸæ¥çš„â€œTop 20 biomarkersâ€è¾“å‡ºï¼šç”¨ç¨³å®šæ€§å‡å€¼æ’å
    st.subheader("ğŸ§¬ Top 20 æ½œåœ¨ç”Ÿç‰©æ ‡å¿—ç‰©ï¼ˆMeanImportance æ’åï¼‰")
    st.dataframe(stability_df.sort_values("MeanImportance", ascending=False).head(20))

    st.download_button(
        "â¬‡ ä¸‹è½½å…¨éƒ¨åŸºå› é‡è¦æ€§ï¼ˆMean/CV/Freqï¼‰",
        stability_df.to_csv(index=False),
        "latent_shap_gene_importance_stability.csv",
    )

    # =====================================================
    # SHAP å¯è§†åŒ–ï¼ˆä½¿ç”¨æœ€åä¸€æ¬¡ run çš„ shap_zï¼‰
    # =====================================================
    if last_shap_z is not None and last_z_test is not None:
        st.subheader("ğŸ” Latent SHAP Summaryï¼ˆdotï¼‰")

        last_shap_z = np.array(last_shap_z)
        if last_shap_z.ndim == 3 and last_shap_z.shape[-1] == 1:
            last_shap_z = last_shap_z[:, :, 0]

        fig1 = plt.figure()
        shap.summary_plot(last_shap_z, features=last_z_test, show=False)
        st.pyplot(fig1)

        st.subheader("ğŸ“Š Latent SHAP Summaryï¼ˆbarï¼‰")
        fig2 = plt.figure()
        shap.summary_plot(last_shap_z, features=last_z_test, plot_type="bar", show=False)
        st.pyplot(fig2)

        abs_latent = np.mean(np.abs(last_shap_z), axis=0)
        latent_df = pd.DataFrame({"LatentDim": np.arange(len(abs_latent)), "MeanAbsSHAP": abs_latent})
        latent_df = latent_df.sort_values("MeanAbsSHAP", ascending=False)

        st.subheader("ğŸ“ˆ Top 20 Latent ç»´åº¦é‡è¦æ€§ï¼ˆMeanAbsSHAPï¼‰")
        st.dataframe(latent_df.head(20))

        fig3 = plt.figure()
        plt.bar(latent_df.head(20)["LatentDim"].astype(str), latent_df.head(20)["MeanAbsSHAP"])
        plt.xticks(rotation=45, ha="right")
        plt.tight_layout()
        st.pyplot(fig3)

        st.download_button(
            "â¬‡ ä¸‹è½½ latent ç»´åº¦ MeanAbsSHAP",
            latent_df.to_csv(index=False),
            "latent_mean_abs_shap.csv",
        )

    # =====================================================
    # Cox ç”Ÿå­˜éªŒè¯ï¼ˆå¯é€‰ï¼‰
    # =====================================================
    if use_survival:
        if surv_file is None:
            st.warning("ä½ å¼€å¯äº† Cox éªŒè¯ï¼Œä½†æ²¡æœ‰ä¸Šä¼ ç”Ÿå­˜æ•°æ®æ–‡ä»¶ã€‚")
        else:
            try:
                from lifelines import CoxPHFitter
                from lifelines.utils import concordance_index
            except Exception:
                st.error("æœªæ£€æµ‹åˆ° lifelinesã€‚è¯·å…ˆå®‰è£…ï¼špip install lifelines")
                st.stop()

            surv = read_csv_cached(surv_file)

            needed = {"Sample", "Time", "Event"}
            if not needed.issubset(set(surv.columns)):
                st.error("ç”Ÿå­˜æ•°æ®å¿…é¡»åŒ…å«åˆ—ï¼šSample, Time, Event")
                st.stop()

            surv["Sample"] = surv["Sample"].astype(str)
            surv = surv.set_index("Sample")

            # å¯¹é½åˆ° RNA æ ·æœ¬
            common = list(set(rna.columns.astype(str)).intersection(set(surv.index.astype(str))))
            if len(common) < 10:
                st.error("ç”Ÿå­˜æ•°æ®ä¸ RNA çš„å…±åŒæ ·æœ¬å¤ªå°‘ï¼ˆ<10ï¼‰ï¼Œæ— æ³•åš Cox éªŒè¯ã€‚")
                st.stop()

            common = [s for s in rna.columns.astype(str) if s in common]  # ä¿æŒ RNA é¡ºåº
            surv_aligned = surv.loc[common]
            rna_aligned = rna[common]

            # é€‰ç¨³å®šåŸºå› 
            freq_col = f"Top{int(top_k)}_Freq"
            selected = stability_df[
                (stability_df[freq_col] >= float(freq_thr)) & (stability_df["CV"] <= float(cv_thr))
            ].sort_values(["MeanImportance"], ascending=False)["Gene"].head(int(max_surv_genes)).tolist()

            st.subheader("â± Cox ç”Ÿå­˜éªŒè¯")
            st.write({
                "å…±åŒæ ·æœ¬æ•°": int(len(common)),
                "ç­›é€‰é˜ˆå€¼": f"{freq_col}â‰¥{freq_thr}, CVâ‰¤{cv_thr}",
                "è¿›å…¥ Cox çš„åŸºå› æ•°": int(len(selected)),
            })

            if len(selected) < 2:
                st.warning("ç­›é€‰ååŸºå› å¤ªå°‘ï¼ˆ<2ï¼‰ã€‚è¯·æ”¾å®½é˜ˆå€¼æˆ–å¢å¤§ top_k / n_runsã€‚")
            else:
                X_surv = rna_aligned.loc[selected].T  # samples x genes

                df_cox = pd.concat([surv_aligned[["Time", "Event"]], X_surv], axis=1).dropna()
                if df_cox.shape[0] < 10:
                    st.warning("å»é™¤ç¼ºå¤±å€¼åæ ·æœ¬å¤ªå°‘ï¼Œæ— æ³•ç¨³å®šæ‹Ÿåˆã€‚")
                else:
                    df_train, df_test = train_test_split(df_cox, test_size=float(test_size), random_state=int(seed_base))

                    cph = CoxPHFitter(penalizer=float(cox_penalizer))
                    cph.fit(df_train, duration_col="Time", event_col="Event")

                    risk = cph.predict_partial_hazard(df_test)
                    c_index = concordance_index(df_test["Time"], -risk.values, df_test["Event"])

                    st.write({"C-index": float(c_index)})

                    st.subheader("ğŸ“Œ Cox å›å½’ç³»æ•° Top 20")
                    coef_df = cph.summary.reset_index().rename(columns={"index": "Feature"})
                    coef_df = coef_df.sort_values("coef", ascending=False)
                    st.dataframe(coef_df.head(20))

                    st.download_button(
                        "â¬‡ ä¸‹è½½ Cox summary",
                        cph.summary.to_csv(),
                        "cox_summary.csv",
                    )

                    # é£é™©åˆ†æ•°å¯¼å‡º
                    out_risk = pd.DataFrame({
                        "Sample": df_test.index.astype(str),
                        "Time": df_test["Time"].values,
                        "Event": df_test["Event"].values,
                        "RiskScore": risk.values.flatten()
                    })
                    st.download_button(
                        "â¬‡ ä¸‹è½½æµ‹è¯•é›†é£é™©åˆ†æ•°ï¼ˆRiskScoreï¼‰",
                        out_risk.to_csv(index=False),
                        "cox_test_risk_scores.csv",
                    )
