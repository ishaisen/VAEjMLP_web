# =====================================================
# VAEjMLP latent-SHAP + ç¨³å®šæ€§ + SHAPå¯è§†åŒ– + CoxéªŒè¯ (Streamlit) â€”â€” å·²ä¿®å¥½ç‰ˆ
# å…³é”®ä¿®å¤ï¼š
# 1) download_button è§¦å‘ rerun åç»“æœä¸æ¶ˆå¤±ï¼šå…¨éƒ¨ç»“æœç¼“å­˜åˆ° st.session_state
# 2) SHAP è¿”å›å½¢çŠ¶å…¼å®¹å¤„ç†ï¼šé¿å… summary_plot å½¢çŠ¶æ–­è¨€æŠ¥é”™
# 3) lifelines ä¸å­˜åœ¨æ—¶ï¼šè‡ªåŠ¨è·³è¿‡ Coxï¼Œä¸ä¸­æ–­ä¸»æµç¨‹
# =====================================================

import streamlit as st
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score

import shap
import matplotlib.pyplot as plt


# =====================================================
# Utils
# =====================================================
def set_seed(seed: int):
    import random

    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)

    # è®©ç»“æœæ›´å¯å¤ç°ï¼ˆå¯èƒ½ç•¥æ…¢ï¼‰
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


@st.cache_data(show_spinner=False)
def read_csv_cached(uploaded_file) -> pd.DataFrame:
    return pd.read_csv(uploaded_file)


def ensure_2d_shap(shap_values, features_2d: np.ndarray) -> np.ndarray:
    """
    å…¼å®¹ shap çš„å¤šç§è¿”å›æ ¼å¼ï¼Œæœ€ç»ˆä¿è¯è¾“å‡º (n_samples, n_features)
    """
    # shap_values å¯èƒ½æ˜¯ list æˆ– ndarray
    if isinstance(shap_values, list):
        shap_z = shap_values[0]
    else:
        shap_z = shap_values

    shap_z = np.array(shap_z)

    # å¯èƒ½æ˜¯ (n, d, 1)
    if shap_z.ndim == 3 and shap_z.shape[-1] == 1:
        shap_z = shap_z[:, :, 0]

    # æœ‰äº›ç‰ˆæœ¬å¯èƒ½ (1, n, d) æˆ–å…¶å®ƒå¥‡æ€ªå½¢çŠ¶ï¼Œåšå…œåº•
    if shap_z.ndim == 3 and shap_z.shape[0] == 1:
        shap_z = shap_z[0]

    if shap_z.ndim != 2:
        raise ValueError(f"Unexpected SHAP shape: {shap_z.shape}")

    if shap_z.shape[0] != features_2d.shape[0] or shap_z.shape[1] != features_2d.shape[1]:
        raise ValueError(
            f"Shape mismatch: shap={shap_z.shape}, features={features_2d.shape}. "
            "Please check mlp_predict output shape and SHAP processing."
        )
    return shap_z


def to_csv_bytes(df: pd.DataFrame) -> bytes:
    return df.to_csv(index=False).encode("utf-8")


# =====================================================
# æ¨¡å‹å®šä¹‰ï¼ˆä¸ä½ åŸå§‹ä¸€è‡´ï¼‰
# =====================================================
class VAE(nn.Module):
    def __init__(self, input_dim, latent_dim):
        super().__init__()
        self.fc1 = nn.Linear(input_dim, 1024)
        self.fc2 = nn.Linear(1024, 512)
        self.fc3 = nn.Linear(512, latent_dim * 2)

    def encode(self, x):
        h1 = F.relu(self.fc1(x))
        h2 = F.relu(self.fc2(h1))
        h3 = self.fc3(h2)
        mean, log_var = torch.chunk(h3, 2, dim=-1)
        return mean, log_var

    def reparameterize(self, mean, log_var):
        std = torch.exp(0.5 * log_var)
        eps = torch.randn_like(std)
        return mean + eps * std

    def forward(self, x):
        mean, log_var = self.encode(x)
        z = self.reparameterize(mean, log_var)
        return z, mean, log_var


class MLP(nn.Module):
    def __init__(self, latent_dim):
        super().__init__()
        self.fc1 = nn.Linear(latent_dim, 128)
        self.fc2 = nn.Linear(128, 64)
        self.fc3 = nn.Linear(64, 1)

    def forward(self, z):
        z = F.relu(self.fc1(z))
        z = F.relu(self.fc2(z))
        return torch.sigmoid(self.fc3(z))


# =====================================================
# Streamlit é¡µé¢
# =====================================================
st.set_page_config(page_title="VAEjMLP latent-SHAP", layout="wide")
st.title("ğŸ§¬ VAEjMLP + latent SHAP ç”Ÿç‰©æ ‡å¿—ç‰©åˆ†æï¼ˆå®Œæ•´æ•´åˆç‰ˆï½œå·²ä¿®å¥½ï¼‰")

# ===== é¡¶éƒ¨å·¥å…·æŒ‰é’®ï¼šæ¸…ç©ºç¼“å­˜ =====
with st.expander("ğŸ§° å·¥å…·", expanded=False):
    if st.button("ğŸ§¹ æ¸…ç©ºç¼“å­˜ç»“æœï¼ˆä¸ä¼šæ¸…ç©ºä¸Šä¼ æ–‡ä»¶ï¼‰"):
        for k in list(st.session_state.keys()):
            if k.startswith("cache_"):
                st.session_state.pop(k, None)
        st.rerun()

with st.sidebar:
    st.header("å‚æ•°è®¾ç½®")

    latent_dim = st.number_input("latent_dim", min_value=4, max_value=1024, value=128, step=4)
    n_epochs = st.number_input("è®­ç»ƒè½®æ•° epochs", min_value=10, max_value=2000, value=100, step=10)
    lr = st.number_input("å­¦ä¹ ç‡ lr", min_value=1e-5, max_value=1e-1, value=1e-3, step=1e-4, format="%.5f")

    ce_weight = st.number_input(
        "CE æƒé‡ï¼ˆloss = KL + ce_weight*CEï¼‰",
        min_value=0.0,
        max_value=10.0,
        value=0.001,
        step=0.001,
    )

    test_size = st.slider("test_size", 0.05, 0.5, 0.2, 0.05)

    st.divider()
    st.subheader("ç¨³å®šæ€§ (multi-run)")

    n_runs = st.slider("é‡å¤è¿è¡Œæ¬¡æ•° n_runs", 1, 50, 10)
    top_k = st.slider("TopK é¢‘ç‡ç»Ÿè®¡", 5, 300, 20)
    seed_base = st.number_input("seed_base", value=42, step=1)

    st.divider()
    st.subheader("SHAP è®¡ç®—")

    background_n = st.slider("background æ ·æœ¬æ•°", 10, 200, 50)
    shap_nsamples = st.slider("KernelExplainer nsamples", 50, 500, 100, 50)
    st.caption("æç¤ºï¼šKernelExplainer å¯èƒ½è¾ƒæ…¢ï¼›n_runs å¤§æ—¶å»ºè®®é™ä½ nsamples æˆ– backgroundã€‚")

    st.divider()
    st.subheader("Cox ç”Ÿå­˜éªŒè¯ï¼ˆå¯é€‰ï¼‰")

    use_survival = st.checkbox("å¯ç”¨ Cox ç”Ÿå­˜éªŒè¯", value=False)
    freq_thr = st.slider("ç¨³å®šåŸºå› ç­›é€‰ï¼šTopK_Freq â‰¥", 0.0, 1.0, 0.6, 0.05)
    cv_thr = st.number_input("ç¨³å®šåŸºå› ç­›é€‰ï¼šCV â‰¤", min_value=0.0, max_value=50.0, value=1.0, step=0.1)
    max_surv_genes = st.slider("æœ€å¤šç”¨äº Cox çš„åŸºå› æ•°", 5, 300, 50)
    cox_penalizer = st.number_input("Cox L2 penalizer", min_value=0.0, max_value=10.0, value=0.1, step=0.1)

st.divider()

rna_file = st.file_uploader("ä¸Šä¼  RNA-seq è¡¨è¾¾çŸ©é˜µï¼ˆgenes Ã— samplesï¼‰ï¼ŒCSVï¼Œindex=Geneï¼Œcolumns=Sample", type="csv")
label_file = st.file_uploader("ä¸Šä¼  Label æ–‡ä»¶ï¼ˆSample, Labelï¼‰ï¼ŒCSV", type="csv")
surv_file = st.file_uploader("ï¼ˆå¯é€‰ï¼‰ä¸Šä¼  ç”Ÿå­˜æ•°æ®ï¼ˆSample, Time, Eventï¼‰ï¼ŒCSV", type="csv") if use_survival else None

run_button = st.button("ğŸš€ è¿è¡Œæ¨¡å‹")


# =====================================================
# è¿è¡Œä¸»æµç¨‹
# =====================================================
if run_button:
    if (rna_file is None) or (label_file is None):
        st.error("è¯·å…ˆä¸Šä¼  RNA è¡¨è¾¾çŸ©é˜µå’Œ Label æ–‡ä»¶ã€‚")
        st.stop()

    with st.spinner("è¯»å–æ•°æ®ä¸­..."):
        rna = read_csv_cached(rna_file)
        labels = read_csv_cached(label_file)

    # ---- æ•°æ®æ ¡éªŒä¸å¯¹é½ ----
    if rna.shape[0] < 2 or rna.shape[1] < 4:
        st.error("RNA çŸ©é˜µç»´åº¦çœ‹èµ·æ¥ä¸å¯¹ï¼šéœ€è¦ genesÃ—samples ä¸”æ ·æœ¬æ•°è‡³å°‘ 4ã€‚")
        st.stop()

    if "Sample" not in labels.columns or "Label" not in labels.columns:
        st.error("Label æ–‡ä»¶å¿…é¡»åŒ…å«åˆ—ï¼šSample, Label")
        st.stop()

    # é»˜è®¤ï¼šrna ç¬¬ä¸€åˆ—ä¸º gene indexï¼ˆå¦‚æœç”¨æˆ·æ²¡è®¾ç½® index_colï¼‰
    if "Unnamed: 0" in rna.columns:
        rna = rna.rename(columns={"Unnamed: 0": "Gene"}).set_index("Gene")

    # å¯¹é½æ ·æœ¬
    samples_rna = list(map(str, rna.columns.tolist()))
    rna.columns = rna.columns.astype(str)
    labels["Sample"] = labels["Sample"].astype(str)

    if set(samples_rna) != set(labels["Sample"].tolist()):
        st.warning("RNA æ ·æœ¬ä¸ Label æ ·æœ¬é›†åˆä¸å®Œå…¨ä¸€è‡´ï¼Œå°†å–äº¤é›†å¯¹é½ã€‚")
        common = sorted(list(set(samples_rna).intersection(set(labels["Sample"].tolist()))))
        if len(common) < 4:
            st.error("å¯¹é½åå…±åŒæ ·æœ¬æ•°å¤ªå°‘ï¼ˆ<4ï¼‰ï¼Œæ— æ³•è®­ç»ƒã€‚")
            st.stop()
        rna = rna[common]
        labels = labels.set_index("Sample").loc[common].reset_index()

    # é‡æ–°æ ¡éªŒé¡ºåºä¸€è‡´
    labels = labels.set_index("Sample").loc[rna.columns].reset_index()

    genes = rna.index.astype(str).tolist()
    y = labels["Label"].values.astype(int)

    # X: samples x genes
    X = MinMaxScaler().fit_transform(rna.T.values)

    # =====================================================
    # å¤šæ¬¡è¿è¡Œï¼šæ”¶é›† metrics / gene_importance / shap_zï¼ˆä»…æœ€åä¸€æ¬¡ç”¨äºç”»å›¾ï¼‰
    # =====================================================
    all_importances = []  # list[pd.Series] index=genes
    topk_lists = []       # list[list[str]]
    metrics_runs = []     # list[dict]
    last_shap_z = None
    last_z_test = None
    last_latent_df = None

    prog = st.progress(0)
    status = st.empty()

    for run_i in range(int(n_runs)):
        seed = int(seed_base + run_i)
        set_seed(seed)

        status.write(f"Run {run_i+1}/{n_runs} | seed={seed}")

        # ---- split ----
        X_train, X_test, y_train, y_test = train_test_split(
            X,
            y,
            test_size=float(test_size),
            random_state=seed,
            stratify=y if len(np.unique(y)) == 2 else None,
        )

        X_train_t = torch.tensor(X_train, dtype=torch.float32)
        X_test_t = torch.tensor(X_test, dtype=torch.float32)
        y_train_t = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)

        # ---- æ¨¡å‹ ----
        vae = VAE(X.shape[1], int(latent_dim))
        mlp = MLP(int(latent_dim))
        optimizer = optim.Adam(list(vae.parameters()) + list(mlp.parameters()), lr=float(lr))

        # ---- è®­ç»ƒ ----
        vae.train()
        mlp.train()
        for _ in range(int(n_epochs)):
            optimizer.zero_grad()
            z, mean, log_var = vae(X_train_t)
            y_pred = mlp(z)
            kl = -0.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp())
            ce = F.binary_cross_entropy(y_pred, y_train_t, reduction="sum")
            loss = kl + float(ce_weight) * ce
            loss.backward()
            optimizer.step()

        vae.eval()
        mlp.eval()

        # ---- é¢„æµ‹ä¸æŒ‡æ ‡ ----
        with torch.no_grad():
            z_test, _, _ = vae(X_test_t)
            y_pred_test = mlp(z_test).cpu().numpy().flatten()

        try:
            auc = roc_auc_score(y_test, y_pred_test)
        except Exception:
            auc = np.nan

        y_hat = (y_pred_test > 0.5).astype(int)

        metrics_runs.append(
            {
                "run": run_i,
                "seed": seed,
                "AUC": auc,
                "Accuracy": accuracy_score(y_test, y_hat),
                "Precision": precision_score(y_test, y_hat, zero_division=0),
                "Recall": recall_score(y_test, y_hat, zero_division=0),
            }
        )

        # =====================================================
        # latent SHAPï¼ˆKernelExplainerï¼‰
        # =====================================================
        with torch.no_grad():
            z_train, _, _ = vae(X_train_t)

        z_train_np = z_train.cpu().numpy()
        z_test_np = z_test.cpu().numpy()

        # â˜… å»ºè®®è¿”å› 1Dï¼Œå‡å°‘ shap è¾“å‡ºæ­§ä¹‰
        def mlp_predict(z_numpy):
            z_t = torch.tensor(z_numpy, dtype=torch.float32)
            with torch.no_grad():
                out = mlp(z_t).cpu().numpy()
            return out.reshape(-1)  # (n,)

        bg_n = int(min(background_n, z_train_np.shape[0]))
        background_z = shap.sample(z_train_np, bg_n)

        explainer = shap.KernelExplainer(mlp_predict, background_z)
        shap_values = explainer.shap_values(z_test_np, nsamples=int(shap_nsamples))
        shap_z = ensure_2d_shap(shap_values, z_test_np)  # (n_test, latent_dim)

        # =====================================================
        # latent â†’ gene æ˜ å°„ï¼ˆä¿æŒä¸ä½ åŸé€»è¾‘ä¸€è‡´ï¼‰
        # =====================================================
        W_gene_hidden = vae.fc1.weight.detach().cpu().numpy()  # (1024, n_genes)
        abs_shap_z = np.mean(np.abs(shap_z), axis=0)          # (latent_dim,)

        gene_importance = {}
        scale = float(np.sum(abs_shap_z))
        for i, gene in enumerate(genes):
            gene_importance[gene] = float(np.mean(np.abs(W_gene_hidden[:, i])) * scale)

        imp_s = pd.Series(gene_importance).reindex(genes)
        all_importances.append(imp_s)
        topk_lists.append(imp_s.sort_values(ascending=False).head(int(top_k)).index.tolist())

        # ä¿å­˜æœ€åä¸€æ¬¡ run çš„ shap ç”¨äºç”»å›¾
        if run_i == int(n_runs) - 1:
            last_shap_z = shap_z
            last_z_test = z_test_np

            abs_latent = np.mean(np.abs(shap_z), axis=0)
            last_latent_df = (
                pd.DataFrame({"LatentDim": np.arange(len(abs_latent)), "MeanAbsSHAP": abs_latent})
                .sort_values("MeanAbsSHAP", ascending=False)
                .reset_index(drop=True)
            )

        prog.progress((run_i + 1) / int(n_runs))

    status.empty()
    prog.empty()

    # =====================================================
    # æ±‡æ€»è¾“å‡ºï¼šæ¨¡å‹æ€§èƒ½ï¼ˆå¤šæ¬¡ runï¼‰
    # =====================================================
    metrics_df = pd.DataFrame(metrics_runs)
    summary_df = metrics_df[["AUC", "Accuracy", "Precision", "Recall"]].agg(["mean", "std"]).T.reset_index()
    summary_df.columns = ["Metric", "Mean", "Std"]

    # =====================================================
    # æ±‡æ€»è¾“å‡ºï¼šåŸºå› é‡è¦æ€§ç¨³å®šæ€§ï¼ˆMean / CV / Frequencyï¼‰
    # =====================================================
    imp_mat = pd.concat(all_importances, axis=1)
    imp_mat.columns = [f"run_{i}" for i in range(int(n_runs))]

    mean_imp = imp_mat.mean(axis=1)
    std_imp = imp_mat.std(axis=1)
    cv_imp = std_imp / (mean_imp.abs() + 1e-12)

    from collections import Counter

    freq_counter = Counter([g for lst in topk_lists for g in lst])
    freq = pd.Series({g: freq_counter.get(g, 0) / float(n_runs) for g in genes})

    freq_col = f"Top{int(top_k)}_Freq"
    stability_df = (
        pd.DataFrame(
            {
                "Gene": genes,
                "MeanImportance": mean_imp.values,
                "StdImportance": std_imp.values,
                "CV": cv_imp.values,
                freq_col: freq.values,
            }
        )
        .sort_values([freq_col, "MeanImportance"], ascending=[False, False])
        .reset_index(drop=True)
    )

    # =====================================================
    # æŠŠç»“æœç¼“å­˜åˆ° session_stateï¼ˆdownload è§¦å‘ rerun ä¹Ÿä¸ä¸¢ï¼‰
    # =====================================================
    st.session_state["cache_metrics_df"] = metrics_df
    st.session_state["cache_summary_df"] = summary_df
    st.session_state["cache_stability_df"] = stability_df
    st.session_state["cache_latent_df"] = last_latent_df

    st.session_state["cache_csv_metrics_all"] = to_csv_bytes(metrics_df)
    st.session_state["cache_csv_summary"] = to_csv_bytes(summary_df)
    st.session_state["cache_csv_stability"] = to_csv_bytes(stability_df)
    st.session_state["cache_csv_latent"] = to_csv_bytes(last_latent_df) if last_latent_df is not None else None

    # ä¹ŸæŠŠå›¾éœ€è¦çš„æ•°ç»„å­˜èµ·æ¥ï¼ˆå¦‚æœä½ ä¸æƒ³å­˜å¤ªå¤§æ•°æ®ï¼Œå¯åˆ é™¤è¿™ä¸¤è¡Œï¼‰
    st.session_state["cache_last_shap_z"] = last_shap_z
    st.session_state["cache_last_z_test"] = last_z_test

    st.success("è¿è¡Œå®Œæˆï¼å·²ç¼“å­˜ç»“æœï¼Œç‚¹å‡»ä»»æ„ä¸‹è½½ä¸ä¼šæ¸…ç©ºã€‚")


# =====================================================
# ç»“æœå±•ç¤ºåŒºï¼šä¼˜å…ˆå±•ç¤ºç¼“å­˜ç»“æœï¼ˆå³ä½¿ download è§¦å‘ rerun ä¹Ÿèƒ½ç»§ç»­æ˜¾ç¤º/ä¸‹è½½ï¼‰
# =====================================================
st.divider()
st.subheader("ğŸ“¦ å½“å‰ç¼“å­˜ç»“æœ")

if "cache_stability_df" not in st.session_state:
    st.write("æš‚æ— ç¼“å­˜ç»“æœã€‚è¯·ä¸Šä¼ æ•°æ®å¹¶ç‚¹å‡»ã€ŒğŸš€ è¿è¡Œæ¨¡å‹ã€ã€‚")
    st.stop()

metrics_df = st.session_state["cache_metrics_df"]
summary_df = st.session_state["cache_summary_df"]
stability_df = st.session_state["cache_stability_df"]
latent_df = st.session_state.get("cache_latent_df", None)

st.subheader("ğŸ“Š æ¨¡å‹æ€§èƒ½ï¼ˆæ¯æ¬¡ runï¼‰")
st.dataframe(metrics_df, use_container_width=True)

st.subheader("ğŸ“Š æ¨¡å‹æ€§èƒ½æ±‡æ€»ï¼ˆå‡å€¼Â±æ ‡å‡†å·®ï¼‰")
st.dataframe(summary_df, use_container_width=True)

c1, c2, c3 = st.columns(3)
with c1:
    st.download_button(
        "â¬‡ ä¸‹è½½æ‰€æœ‰ run çš„æ¨¡å‹æŒ‡æ ‡",
        st.session_state.get("cache_csv_metrics_all", b""),
        "model_metrics_all_runs.csv",
        mime="text/csv",
    )
with c2:
    st.download_button(
        "â¬‡ ä¸‹è½½æŒ‡æ ‡æ±‡æ€»ï¼ˆå‡å€¼Â±æ ‡å‡†å·®ï¼‰",
        st.session_state.get("cache_csv_summary", b""),
        "model_metrics_summary.csv",
        mime="text/csv",
    )
with c3:
    # å…¼å®¹ä½ åŸæ¥æ–‡ä»¶å
    st.download_button(
        "â¬‡ ä¸‹è½½å…¨éƒ¨åŸºå› é‡è¦æ€§ï¼ˆMean/CV/Freqï¼‰",
        st.session_state.get("cache_csv_stability", b""),
        "latent_shap_gene_importance_stability.csv",
        mime="text/csv",
    )

st.subheader("ğŸ“Œ ç”Ÿç‰©æ ‡å¿—ç‰©ç¨³å®šæ€§ï¼ˆFrequency / CVï¼‰")
st.dataframe(stability_df.head(50), use_container_width=True)

st.download_button(
    "â¬‡ ä¸‹è½½ç¨³å®šæ€§ç»Ÿè®¡è¡¨",
    st.session_state.get("cache_csv_stability", b""),
    "biomarker_stability.csv",
    mime="text/csv",
)

st.subheader("ğŸ§¬ Top 20 æ½œåœ¨ç”Ÿç‰©æ ‡å¿—ç‰©ï¼ˆMeanImportance æ’åï¼‰")
st.dataframe(stability_df.sort_values("MeanImportance", ascending=False).head(20), use_container_width=True)

# =====================================================
# SHAP å¯è§†åŒ–ï¼ˆç”¨ç¼“å­˜çš„æœ€åä¸€æ¬¡ runï¼‰
# =====================================================
last_shap_z = st.session_state.get("cache_last_shap_z", None)
last_z_test = st.session_state.get("cache_last_z_test", None)

if (last_shap_z is not None) and (last_z_test is not None):
    st.divider()
    st.subheader("ğŸ” Latent SHAP Summaryï¼ˆdotï¼‰")
    fig1 = plt.figure()
    shap.summary_plot(last_shap_z, features=last_z_test, show=False)
    st.pyplot(fig1)

    st.subheader("ğŸ“Š Latent SHAP Summaryï¼ˆbarï¼‰")
    fig2 = plt.figure()
    shap.summary_plot(last_shap_z, features=last_z_test, plot_type="bar", show=False)
    st.pyplot(fig2)

    if latent_df is not None:
        st.subheader("ğŸ“ˆ Top 20 Latent ç»´åº¦é‡è¦æ€§ï¼ˆMeanAbsSHAPï¼‰")
        st.dataframe(latent_df.head(20), use_container_width=True)

        fig3 = plt.figure()
        plt.bar(latent_df.head(20)["LatentDim"].astype(str), latent_df.head(20)["MeanAbsSHAP"])
        plt.xticks(rotation=45, ha="right")
        plt.tight_layout()
        st.pyplot(fig3)

        csv_latent = st.session_state.get("cache_csv_latent", None)
        if csv_latent is not None:
            st.download_button(
                "â¬‡ ä¸‹è½½ latent ç»´åº¦ MeanAbsSHAP",
                csv_latent,
                "latent_mean_abs_shap.csv",
                mime="text/csv",
            )

# =====================================================
# Cox ç”Ÿå­˜éªŒè¯ï¼ˆå¯é€‰ï¼‰ï¼šlifelines ç¼ºå¤±åˆ™è·³è¿‡ï¼Œä¸ä¸­æ–­
# =====================================================
if use_survival:
    st.divider()
    st.subheader("â± Cox ç”Ÿå­˜éªŒè¯ï¼ˆå¯é€‰ï¼‰")

    if surv_file is None:
        st.warning("ä½ å¼€å¯äº† Cox éªŒè¯ï¼Œä½†æ²¡æœ‰ä¸Šä¼ ç”Ÿå­˜æ•°æ®æ–‡ä»¶ã€‚")
    else:
        try:
            from lifelines import CoxPHFitter
            from lifelines.utils import concordance_index

            lifelines_ok = True
        except Exception:
            lifelines_ok = False

        if not lifelines_ok:
            st.warning("æœªæ£€æµ‹åˆ° lifelinesï¼Œå·²è‡ªåŠ¨è·³è¿‡ Cox éªŒè¯ï¼ˆå¯ç”¨ pip install lifelines å¯ç”¨ï¼‰ã€‚")
        else:
            # é‡æ–°è¯»å–æ–‡ä»¶ï¼ˆdownload è§¦å‘ rerun æ—¶ï¼Œsurv_file ä»å¯èƒ½å­˜åœ¨ï¼‰
            surv = read_csv_cached(surv_file)

            needed = {"Sample", "Time", "Event"}
            if not needed.issubset(set(surv.columns)):
                st.error("ç”Ÿå­˜æ•°æ®å¿…é¡»åŒ…å«åˆ—ï¼šSample, Time, Event")
            else:
                surv["Sample"] = surv["Sample"].astype(str)
                surv = surv.set_index("Sample")

                # æ³¨æ„ï¼šrna åªåœ¨ run_button æ—¶å­˜åœ¨ï¼›è¿™é‡Œä»ä¸Šä¼ æ–‡ä»¶é‡æ–°è¯»ä¸€éä»¥ä¿è¯ç‹¬ç«‹
                if (rna_file is None):
                    st.warning("å½“å‰ä¼šè¯æœªæ£€æµ‹åˆ° RNA æ–‡ä»¶ä¸Šä¼ ï¼ˆæˆ–å·²åˆ·æ–°ï¼‰ã€‚è¯·é‡æ–°ä¸Šä¼  RNA æ–‡ä»¶ä»¥è¿›è¡Œ Cox éªŒè¯ã€‚")
                else:
                    rna_tmp = read_csv_cached(rna_file)
                    if "Unnamed: 0" in rna_tmp.columns:
                        rna_tmp = rna_tmp.rename(columns={"Unnamed: 0": "Gene"}).set_index("Gene")
                    rna_tmp.columns = rna_tmp.columns.astype(str)

                    common = list(set(rna_tmp.columns).intersection(set(surv.index)))
                    if len(common) < 10:
                        st.error("ç”Ÿå­˜æ•°æ®ä¸ RNA çš„å…±åŒæ ·æœ¬å¤ªå°‘ï¼ˆ<10ï¼‰ï¼Œæ— æ³•åš Cox éªŒè¯ã€‚")
                    else:
                        # ä¿æŒ RNA é¡ºåº
                        common = [s for s in rna_tmp.columns if s in common]
                        surv_aligned = surv.loc[common]
                        rna_aligned = rna_tmp[common]

                        freq_col = f"Top{int(top_k)}_Freq"
                        selected = (
                            stability_df[
                                (stability_df[freq_col] >= float(freq_thr)) & (stability_df["CV"] <= float(cv_thr))
                            ]
                            .sort_values("MeanImportance", ascending=False)["Gene"]
                            .head(int(max_surv_genes))
                            .tolist()
                        )

                        st.write(
                            {
                                "å…±åŒæ ·æœ¬æ•°": int(len(common)),
                                "ç­›é€‰é˜ˆå€¼": f"{freq_col}â‰¥{freq_thr}, CVâ‰¤{cv_thr}",
                                "è¿›å…¥ Cox çš„åŸºå› æ•°": int(len(selected)),
                            }
                        )

                        if len(selected) < 2:
                            st.warning("ç­›é€‰ååŸºå› å¤ªå°‘ï¼ˆ<2ï¼‰ã€‚è¯·æ”¾å®½é˜ˆå€¼æˆ–å¢å¤§ top_k / n_runsã€‚")
                        else:
                            X_surv = rna_aligned.loc[selected].T  # samples x genes
                            df_cox = pd.concat([surv_aligned[["Time", "Event"]], X_surv], axis=1).dropna()

                            if df_cox.shape[0] < 10:
                                st.warning("å»é™¤ç¼ºå¤±å€¼åæ ·æœ¬å¤ªå°‘ï¼Œæ— æ³•ç¨³å®šæ‹Ÿåˆã€‚")
                            else:
                                df_train, df_test = train_test_split(
                                    df_cox, test_size=float(test_size), random_state=int(seed_base)
                                )

                                cph = CoxPHFitter(penalizer=float(cox_penalizer))
                                cph.fit(df_train, duration_col="Time", event_col="Event")

                                risk = cph.predict_partial_hazard(df_test)
                                c_index = concordance_index(df_test["Time"], -risk.values, df_test["Event"])
                                st.write({"C-index": float(c_index)})

                                st.subheader("ğŸ“Œ Cox å›å½’ç³»æ•° Top 20")
                                coef_df = cph.summary.reset_index().rename(columns={"index": "Feature"})
                                coef_df = coef_df.sort_values("coef", ascending=False)
                                st.dataframe(coef_df.head(20), use_container_width=True)

                                st.download_button(
                                    "â¬‡ ä¸‹è½½ Cox summary",
                                    cph.summary.to_csv().encode("utf-8"),
                                    "cox_summary.csv",
                                    mime="text/csv",
                                )

                                out_risk = pd.DataFrame(
                                    {
                                        "Sample": df_test.index.astype(str),
                                        "Time": df_test["Time"].values,
                                        "Event": df_test["Event"].values,
                                        "RiskScore": risk.values.flatten(),
                                    }
                                )
                                st.download_button(
                                    "â¬‡ ä¸‹è½½æµ‹è¯•é›†é£é™©åˆ†æ•°ï¼ˆRiskScoreï¼‰",
                                    out_risk.to_csv(index=False).encode("utf-8"),
                                    "cox_test_risk_scores.csv",
                                    mime="text/csv",
                                )
