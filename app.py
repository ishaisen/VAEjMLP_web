# =====================================================
# app.py
# VAEjMLP latent-SHAP + ç¨³å®šæ€§ + SHAPå¯è§†åŒ– + CoxéªŒè¯ + Top20:
#   â‘  ä¸»æµç¨‹ï¼šVAE+MLPã€å¤šæ¬¡ run ç¨³å®šæ€§ï¼ˆFreq/CVï¼‰ã€Top20ã€latent SHAP
#   â‘¡ ä¸‹è½½åŒºï¼šæ‰€æœ‰ç»“æœæŒä¹…åŒ–ï¼ˆdownload ä¸æ¸…ç©ºï¼‰
#   â‘¢ GO/KEGG å¯Œé›†ï¼ˆgseapy.enrichr, éœ€è”ç½‘ï¼‰
#   â‘£ å·®å¼‚åˆ†æï¼ˆlabels ä¸¤ç»„ï¼ŒWelch t-test + FDR + ç«å±±å›¾ + ç®±çº¿å›¾ï¼‰
#   â‘¤ èšç±»ï¼ˆTop20 è¡¨è¾¾ï¼‰+ èšç±»åˆ†ç»„ç”Ÿå­˜ï¼ˆKM/logrank/Coxï¼‰
#
# âœ… å·²ä¿®å¤ï¼šå·®å¼‚åˆ†ææŠ¥é”™ KeyError 'Sample'
#   - clean_columns() å» BOM/ç©ºç™½
#   - normalize_labels_df() å¼ºåˆ¶è¾“å‡º Sample/Label å¹¶æ¸…æ´—åˆ—å
#   - compute_de_top_genes() ä½¿ç”¨ reindex å¯¹é½ï¼Œé¿å… loc KeyError
#
# ä¾èµ–ï¼š
#   åŸºç¡€ï¼šstreamlit pandas numpy torch scikit-learn shap matplotlib
#   å·®å¼‚ï¼šscipy statsmodels
#   å¯Œé›†ï¼šgseapyï¼ˆéœ€è¦å¤–ç½‘è®¿é—® Enrichrï¼‰
#   ç”Ÿå­˜ï¼šlifelines
# =====================================================

import streamlit as st
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score
from sklearn.cluster import KMeans

import shap
import matplotlib.pyplot as plt

# ----------------- Optional deps -----------------
try:
    from scipy.stats import ttest_ind
    SCIPY_OK = True
except Exception:
    SCIPY_OK = False

try:
    from statsmodels.stats.multitest import multipletests
    STATSMODELS_OK = True
except Exception:
    STATSMODELS_OK = False

try:
    import gseapy as gp
    GSEAPY_OK = True
except Exception:
    GSEAPY_OK = False

try:
    from lifelines import CoxPHFitter, KaplanMeierFitter
    from lifelines.statistics import logrank_test
    from lifelines.utils import concordance_index
    LIFELINES_OK = True
except Exception:
    LIFELINES_OK = False


# =====================================================
# Utils
# =====================================================
def set_seed(seed: int):
    import random

    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


@st.cache_data(show_spinner=False)
def read_csv_cached(uploaded_file) -> pd.DataFrame:
    return pd.read_csv(uploaded_file)


def to_csv_bytes(df: pd.DataFrame) -> bytes:
    return df.to_csv(index=False).encode("utf-8")


def safe_rename_index_col(df: pd.DataFrame) -> pd.DataFrame:
    # å…¼å®¹ï¼šç”¨æˆ·æ²¡è®¾ç½® index_col å¯¼è‡´å‡ºç° Unnamed: 0
    if "Unnamed: 0" in df.columns:
        df = df.rename(columns={"Unnamed: 0": "Gene"}).set_index("Gene")
    return df


def _norm(s: str) -> str:
    return str(s).strip().lower().replace(" ", "").replace("-", "").replace("_", "")


def clean_columns(df: pd.DataFrame) -> pd.DataFrame:
    """
    âœ… å» BOM/ç©ºç™½ï¼Œé¿å…å‡ºç°è‚‰çœ¼çœ‹æ˜¯ Sample å®é™…æ˜¯ \ufeffSample çš„ KeyError
    """
    df = df.copy()
    df.columns = [str(c).replace("\ufeff", "").strip() for c in df.columns]
    return df


def normalize_labels_df(labels_raw: pd.DataFrame) -> pd.DataFrame:
    """
    âœ… å¼ºåˆ¶è¾“å‡ºä¸¤åˆ—ï¼šSample, Label
    - è‡ªåŠ¨è¯†åˆ«åˆ—å
    - å» BOM/ç©ºç™½
    """
    labels = clean_columns(labels_raw)

    if labels.shape[1] < 2:
        raise ValueError("Label æ–‡ä»¶è‡³å°‘éœ€è¦ä¸¤åˆ—ï¼ˆæ ·æœ¬åˆ— + æ ‡ç­¾åˆ—ï¼‰ã€‚")

    col_norm_map = {_norm(c): c for c in labels.columns}

    sample_alias = ["sample", "sampleid", "sample_id", "id", "patient", "patientid", "subject", "subjectid"]
    label_alias = ["label", "group", "class", "y", "target", "phenotype", "status", "casecontrol", "case_control"]

    sample_col = None
    label_col = None

    for a in sample_alias:
        key = _norm(a)
        if key in col_norm_map:
            sample_col = col_norm_map[key]
            break

    for a in label_alias:
        key = _norm(a)
        if key in col_norm_map:
            label_col = col_norm_map[key]
            break

    if sample_col is None:
        sample_col = labels.columns[0]
    if label_col is None:
        label_col = labels.columns[1] if labels.columns[1] != sample_col else labels.columns[0]

    labels = labels.rename(columns={sample_col: "Sample", label_col: "Label"})
    labels["Sample"] = labels["Sample"].astype(str).str.strip()
    labels["Label"] = labels["Label"]

    return labels[["Sample", "Label"]]


def align_rna_labels(rna_raw: pd.DataFrame, labels_raw: pd.DataFrame):
    """
    rna: genes x samples
    labels: Sample, Label
    è¿”å›ï¼šå¯¹é½åçš„ rna, labelsï¼ˆé¡ºåºä¸¥æ ¼ä¸ rna.columns ä¸€è‡´ï¼‰
    """
    rna = safe_rename_index_col(rna_raw.copy())
    rna = clean_columns(rna)
    rna.columns = rna.columns.astype(str)

    labels = normalize_labels_df(labels_raw)

    samples_rna = set(rna.columns.tolist())
    samples_lab = set(labels["Sample"].tolist())
    common = sorted(list(samples_rna.intersection(samples_lab)))
    if len(common) < 4:
        raise ValueError(f"RNA ä¸ Label äº¤é›†æ ·æœ¬æ•°å¤ªå°‘ï¼ˆ{len(common)}ï¼‰ï¼Œæ— æ³•è®­ç»ƒã€‚")

    if samples_rna != samples_lab:
        st.warning("RNA æ ·æœ¬ä¸ Label æ ·æœ¬é›†åˆä¸å®Œå…¨ä¸€è‡´ï¼Œå°†å–äº¤é›†å¯¹é½ã€‚")
        rna = rna[common]
        labels = labels.set_index("Sample").loc[common].reset_index()

    # ä¿è¯é¡ºåºä¸€è‡´
    labels = labels.set_index("Sample").loc[rna.columns].reset_index()

    # ä¿é™©ï¼šåˆ—åå¿…é¡»å­˜åœ¨
    labels = clean_columns(labels)
    if "Sample" not in labels.columns:
        labels = labels.rename(columns={labels.columns[0]: "Sample"})
    if "Label" not in labels.columns:
        raise ValueError("labels ä¸­æœªæ‰¾åˆ° Label åˆ—ï¼Œè¯·æ£€æŸ¥ä¸Šä¼ æ–‡ä»¶ã€‚")

    return rna, labels


def ensure_2d_shap(shap_values, features_2d: np.ndarray) -> np.ndarray:
    """
    å…¼å®¹ shap å¤šç‰ˆæœ¬è¿”å›å€¼ï¼š
    - list([array])
    - array
    - (n,d,1) / (1,n,d) ç­‰
    æœ€ç»ˆè¾“å‡º (n,d)
    """
    if isinstance(shap_values, list):
        shap_z = shap_values[0]
    else:
        shap_z = shap_values

    shap_z = np.array(shap_z)

    if shap_z.ndim == 3 and shap_z.shape[-1] == 1:
        shap_z = shap_z[:, :, 0]
    if shap_z.ndim == 3 and shap_z.shape[0] == 1:
        shap_z = shap_z[0]

    if shap_z.ndim != 2:
        raise ValueError(f"Unexpected SHAP shape: {shap_z.shape}")

    if shap_z.shape[0] != features_2d.shape[0] or shap_z.shape[1] != features_2d.shape[1]:
        raise ValueError(f"Shape mismatch: shap={shap_z.shape}, features={features_2d.shape}")

    return shap_z


def compute_de_top_genes(rna: pd.DataFrame, labels: pd.DataFrame, top_genes: list):
    """
    âœ… ä¸¤ç»„å·®å¼‚ï¼šWelch t-test + log2FC + BH-FDR
    âœ… ä¸å†ç”¨ loc[rna.columns] ç›´æ¥ç´¢å¼•ï¼ˆå®¹æ˜“ KeyErrorï¼‰ï¼Œæ”¹ç”¨ reindex
    """
    if not SCIPY_OK:
        raise RuntimeError("ç¼ºå°‘ scipyï¼Œæ— æ³•åš t-testã€‚è¯·å®‰è£…ï¼špip install scipy")
    if not STATSMODELS_OK:
        raise RuntimeError("ç¼ºå°‘ statsmodelsï¼Œæ— æ³•åš FDRã€‚è¯·å®‰è£…ï¼špip install statsmodels")

    rna = clean_columns(rna.copy())
    rna.columns = rna.columns.astype(str)

    lab = clean_columns(labels.copy())
    if ("Sample" not in lab.columns) or ("Label" not in lab.columns):
        lab = normalize_labels_df(lab)
    else:
        lab["Sample"] = lab["Sample"].astype(str).str.strip()

    # âœ… ç”¨ reindex å¯¹é½ï¼Œé¿å… KeyError
    lab2 = lab.set_index("Sample").reindex(rna.columns)

    missing = lab2.index[lab2["Label"].isna()].tolist()
    if len(missing) > 0:
        raise ValueError(f"labels ä¸­ç¼ºå°‘ {len(missing)} ä¸ªæ ·æœ¬çš„æ ‡ç­¾ï¼ˆç¤ºä¾‹å‰5ä¸ªï¼‰ï¼š{missing[:5]}")

    groups = pd.Series(lab2["Label"].values).unique().tolist()
    if len(groups) != 2:
        raise ValueError(f"å·®å¼‚åˆ†æéœ€è¦ä¸¤ç»„ Labelï¼Œç›®å‰å‘ç° {len(groups)} ç»„ï¼š{groups}")

    g0, g1 = groups[0], groups[1]
    s0 = lab2[lab2["Label"] == g0].index.tolist()
    s1 = lab2[lab2["Label"] == g1].index.tolist()

    if len(s0) < 2 or len(s1) < 2:
        raise ValueError(f"ä¸¤ç»„æ ·æœ¬æ•°ä¸è¶³ï¼š{g0}={len(s0)}, {g1}={len(s1)}ï¼ˆæ¯ç»„è‡³å°‘ 2ï¼‰")

    eps = 1e-9
    rows = []
    for gene in top_genes:
        if gene not in rna.index:
            continue
        x0 = rna.loc[gene].reindex(s0).astype(float).values
        x1 = rna.loc[gene].reindex(s1).astype(float).values
        stat, p = ttest_ind(x1, x0, equal_var=False, nan_policy="omit")
        m0 = np.nanmean(x0)
        m1 = np.nanmean(x1)
        log2fc = np.log2((m1 + eps) / (m0 + eps))
        rows.append([gene, m0, m1, log2fc, p])

    de = pd.DataFrame(rows, columns=["Gene", f"Mean({g0})", f"Mean({g1})", "log2FC", "p_value"])
    if len(de) == 0:
        raise ValueError("Top genes åœ¨ RNA ä¸­æœªåŒ¹é…åˆ°ä»»ä½•åŸºå› ã€‚")

    de["FDR"] = multipletests(de["p_value"].values, method="fdr_bh")[1]
    de = de.sort_values(["FDR", "p_value"], ascending=True).reset_index(drop=True)
    return de, (g0, g1)


def run_enrichr(top_genes: list, organism: str = "Human"):
    """
    GO/KEGG via Enrichr (gseapy.enrichr) â€”â€” éœ€è¦è”ç½‘
    """
    if not GSEAPY_OK:
        raise RuntimeError("ç¼ºå°‘ gseapyï¼Œæ— æ³•åš GO/KEGGã€‚è¯·å®‰è£…ï¼špip install gseapy")

    if organism.lower().startswith("h"):
        libs = [
            "GO_Biological_Process_2021",
            "GO_Molecular_Function_2021",
            "GO_Cellular_Component_2021",
            "KEGG_2021_Human",
        ]
    else:
        libs = [
            "GO_Biological_Process_2021",
            "GO_Molecular_Function_2021",
            "GO_Cellular_Component_2021",
            "KEGG_2021_Mouse",
        ]

    out = {}
    for lib in libs:
        enr = gp.enrichr(gene_list=top_genes, gene_sets=lib, organism=organism, outdir=None)
        out[lib] = enr.results.copy()
    return out


def cluster_samples_by_top_genes(rna: pd.DataFrame, top_genes: list, n_clusters: int = 2, seed: int = 42):
    """
    åŸºäº Top genes è¡¨è¾¾åš KMeans èšç±»
    """
    rna = clean_columns(rna.copy())
    rna.columns = rna.columns.astype(str)

    genes_exist = [g for g in top_genes if g in rna.index]
    if len(genes_exist) < 2:
        raise ValueError("Top genes åœ¨ RNA ä¸­åŒ¹é…åˆ°çš„åŸºå› å¤ªå°‘ï¼ˆ<2ï¼‰ï¼Œæ— æ³•èšç±»ã€‚")

    X = rna.loc[genes_exist].T.astype(float)
    X_scaled = StandardScaler().fit_transform(X.values)

    km = KMeans(n_clusters=int(n_clusters), random_state=int(seed), n_init="auto")
    clusters = km.fit_predict(X_scaled)

    cluster_df = pd.DataFrame({"Sample": X.index.astype(str), "Cluster": clusters.astype(int)})
    X_scaled_df = pd.DataFrame(X_scaled, index=X.index.astype(str), columns=genes_exist)
    return cluster_df, X_scaled_df


def km_plot_by_group(surv_df: pd.DataFrame, group_col: str, time_col: str = "Time", event_col: str = "Event"):
    if not LIFELINES_OK:
        raise RuntimeError("ç¼ºå°‘ lifelinesï¼Œæ— æ³•åš KM/Coxã€‚è¯·å®‰è£…ï¼špip install lifelines")

    fig = plt.figure()
    kmf = KaplanMeierFitter()
    groups = sorted(surv_df[group_col].unique().tolist())

    for g in groups:
        dfg = surv_df[surv_df[group_col] == g]
        kmf.fit(durations=dfg[time_col], event_observed=dfg[event_col], label=f"{group_col}={g} (n={len(dfg)})")
        kmf.plot(ci_show=False)

    plt.title("Kaplan-Meier")
    plt.xlabel("Time")
    plt.ylabel("Survival probability")
    plt.tight_layout()

    p_lr = None
    if len(groups) == 2:
        g0, g1 = groups
        df0 = surv_df[surv_df[group_col] == g0]
        df1 = surv_df[surv_df[group_col] == g1]
        lr = logrank_test(
            df0[time_col], df1[time_col],
            event_observed_A=df0[event_col],
            event_observed_B=df1[event_col],
        )
        p_lr = float(lr.p_value)

    return fig, p_lr


# =====================================================
# Model
# =====================================================
class VAE(nn.Module):
    def __init__(self, input_dim, latent_dim):
        super().__init__()
        self.fc1 = nn.Linear(input_dim, 1024)
        self.fc2 = nn.Linear(1024, 512)
        self.fc3 = nn.Linear(512, latent_dim * 2)

    def encode(self, x):
        h1 = F.relu(self.fc1(x))
        h2 = F.relu(self.fc2(h1))
        h3 = self.fc3(h2)
        mean, log_var = torch.chunk(h3, 2, dim=-1)
        return mean, log_var

    def reparameterize(self, mean, log_var):
        std = torch.exp(0.5 * log_var)
        eps = torch.randn_like(std)
        return mean + eps * std

    def forward(self, x):
        mean, log_var = self.encode(x)
        z = self.reparameterize(mean, log_var)
        return z, mean, log_var


class MLP(nn.Module):
    def __init__(self, latent_dim):
        super().__init__()
        self.fc1 = nn.Linear(latent_dim, 128)
        self.fc2 = nn.Linear(128, 64)
        self.fc3 = nn.Linear(64, 1)

    def forward(self, z):
        z = F.relu(self.fc1(z))
        z = F.relu(self.fc2(z))
        return torch.sigmoid(self.fc3(z))


# =====================================================
# Page + Navigation
# =====================================================
st.set_page_config(page_title="VAEjMLP latent-SHAP BioApp", layout="wide")
st.title("ğŸ§¬ VAEjMLP + latent SHAP ç”Ÿç‰©æ ‡å¿—ç‰©åˆ†æ")

with st.expander("ğŸ§° å·¥å…·", expanded=False):
    if st.button("ğŸ§¹ æ¸…ç©ºç¼“å­˜ç»“æœï¼ˆä¸ä¼šæ¸…ç©ºä¸Šä¼ æ–‡ä»¶ï¼‰"):
        for k in list(st.session_state.keys()):
            if k.startswith("cache_"):
                st.session_state.pop(k, None)
        st.rerun()

with st.sidebar:
    st.header("å¯¼èˆª")
    module = st.radio(
        "é€‰æ‹©æ¨¡å—",
        [
            "â‘  è®­ç»ƒ/SHAP/ç¨³å®šæ€§ï¼ˆä¸»æµç¨‹ï¼‰",
            "â‘¡ ç»“æœä¸‹è½½ä¸å›æ˜¾",
            "â‘¢ GO/KEGG å¯Œé›†åˆ†æï¼ˆTop20ï¼‰",
            "â‘£ å·®å¼‚åˆ†æï¼ˆlabels åˆ†ç»„ï¼ŒTop20ï¼‰",
            "â‘¤ èšç±»ï¼ˆTop20ï¼‰+ ç”Ÿå­˜åˆ†æï¼ˆKM/Coxï¼‰",
        ],
        index=0,
    )

    st.divider()
    st.header("ä¸»æµç¨‹å‚æ•°")
    latent_dim = st.number_input("latent_dim", min_value=4, max_value=1024, value=128, step=4)
    n_epochs = st.number_input("è®­ç»ƒè½®æ•° epochs", min_value=10, max_value=2000, value=100, step=10)
    lr = st.number_input("å­¦ä¹ ç‡ lr", min_value=1e-5, max_value=1e-1, value=1e-3, step=1e-4, format="%.5f")
    ce_weight = st.number_input("CE æƒé‡ï¼ˆloss = KL + ce_weight*CEï¼‰", min_value=0.0, max_value=10.0, value=0.001, step=0.001)
    test_size = st.slider("test_size", 0.05, 0.5, 0.2, 0.05)

    st.subheader("ç¨³å®šæ€§ (multi-run)")
    n_runs = st.slider("é‡å¤è¿è¡Œæ¬¡æ•° n_runs", 1, 50, 10)
    top_k = st.slider("TopK é¢‘ç‡ç»Ÿè®¡", 5, 300, 20)
    seed_base = st.number_input("seed_base", value=42, step=1)

    st.subheader("SHAP è®¡ç®—")
    background_n = st.slider("background æ ·æœ¬æ•°", 10, 200, 50)
    shap_nsamples = st.slider("KernelExplainer nsamples", 50, 500, 100, 50)

    st.divider()
    st.header("èšç±»/ç”Ÿå­˜å‚æ•°")
    cluster_k = st.slider("èšç±»ç°‡æ•° K", 2, 6, 2)
    cox_penalizer = st.number_input("Cox L2 penalizer", min_value=0.0, max_value=10.0, value=0.1, step=0.1)


# =====================================================
# Upload area (global)
# =====================================================
st.divider()
u1, u2, u3 = st.columns([1, 1, 1])
with u1:
    rna_file = st.file_uploader("ä¸Šä¼  RNA-seqï¼ˆgenesÃ—samplesï¼‰CSV", type="csv", key="rna_uploader")
with u2:
    label_file = st.file_uploader("ä¸Šä¼  Label CSVï¼ˆåˆ—åå¯ä¸åŒï¼Œä¼šè‡ªåŠ¨è¯†åˆ« Sample/Labelï¼‰", type="csv", key="label_uploader")
with u3:
    surv_file = st.file_uploader("ä¸Šä¼  ç”Ÿå­˜ CSVï¼ˆSample, Time, Eventï¼Œå¯é€‰ï¼‰", type="csv", key="surv_uploader")

run_button = st.button("ğŸš€ è¿è¡Œä¸»æµç¨‹ï¼ˆè®­ç»ƒ + SHAP + ç¨³å®šæ€§ï¼‰", type="primary")


# =====================================================
# Main pipeline
# =====================================================
if run_button:
    if rna_file is None or label_file is None:
        st.error("è¯·å…ˆä¸Šä¼  RNA è¡¨è¾¾çŸ©é˜µå’Œ Label æ–‡ä»¶ã€‚")
        st.stop()

    with st.spinner("è¯»å–å¹¶å¯¹é½æ•°æ®..."):
        rna_raw = read_csv_cached(rna_file)
        labels_raw = read_csv_cached(label_file)
        try:
            rna, labels = align_rna_labels(rna_raw, labels_raw)
        except Exception as e:
            st.error(f"æ•°æ®å¯¹é½å¤±è´¥ï¼š{e}")
            st.stop()

    if rna.shape[0] < 2 or rna.shape[1] < 4:
        st.error("RNA çŸ©é˜µç»´åº¦ä¸å¯¹ï¼šéœ€è¦ genesÃ—samples ä¸”æ ·æœ¬æ•°è‡³å°‘ 4ã€‚")
        st.stop()

    genes = rna.index.astype(str).tolist()
    y = labels["Label"].values

    X = MinMaxScaler().fit_transform(rna.T.values)

    all_importances = []
    topk_lists = []
    metrics_runs = []
    last_shap_z = None
    last_z_test = None
    last_latent_df = None

    prog = st.progress(0)
    status = st.empty()

    for run_i in range(int(n_runs)):
        seed = int(seed_base + run_i)
        set_seed(seed)
        status.write(f"Run {run_i+1}/{n_runs} | seed={seed}")

        X_train, X_test, y_train, y_test = train_test_split(
            X,
            y,
            test_size=float(test_size),
            random_state=seed,
            stratify=y if len(pd.Series(y).unique()) == 2 else None,
        )

        X_train_t = torch.tensor(X_train, dtype=torch.float32)
        X_test_t = torch.tensor(X_test, dtype=torch.float32)
        y_train_t = torch.tensor(pd.Series(y_train).astype(float).values, dtype=torch.float32).view(-1, 1)

        vae = VAE(X.shape[1], int(latent_dim))
        mlp = MLP(int(latent_dim))
        optimizer = optim.Adam(list(vae.parameters()) + list(mlp.parameters()), lr=float(lr))

        vae.train()
        mlp.train()
        for _ in range(int(n_epochs)):
            optimizer.zero_grad()
            z, mean, log_var = vae(X_train_t)
            y_pred = mlp(z)
            kl = -0.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp())
            ce = F.binary_cross_entropy(y_pred, y_train_t, reduction="sum")
            loss = kl + float(ce_weight) * ce
            loss.backward()
            optimizer.step()

        vae.eval()
        mlp.eval()

        with torch.no_grad():
            z_test, _, _ = vae(X_test_t)
            y_pred_test = mlp(z_test).cpu().numpy().flatten()

        try:
            auc = roc_auc_score(y_test, y_pred_test)
        except Exception:
            auc = np.nan

        y_hat = (y_pred_test > 0.5).astype(int)

        metrics_runs.append(
            {
                "run": run_i,
                "seed": seed,
                "AUC": auc,
                "Accuracy": accuracy_score(y_test, y_hat),
                "Precision": precision_score(y_test, y_hat, zero_division=0),
                "Recall": recall_score(y_test, y_hat, zero_division=0),
            }
        )

        with torch.no_grad():
            z_train, _, _ = vae(X_train_t)

        z_train_np = z_train.cpu().numpy()
        z_test_np = z_test.cpu().numpy()

        def mlp_predict(z_numpy):
            z_t = torch.tensor(z_numpy, dtype=torch.float32)
            with torch.no_grad():
                out = mlp(z_t).cpu().numpy()
            return out.reshape(-1)

        bg_n = int(min(background_n, z_train_np.shape[0]))
        background_z = shap.sample(z_train_np, bg_n)

        explainer = shap.KernelExplainer(mlp_predict, background_z)
        shap_values = explainer.shap_values(z_test_np, nsamples=int(shap_nsamples))
        shap_z = ensure_2d_shap(shap_values, z_test_np)

        W_gene_hidden = vae.fc1.weight.detach().cpu().numpy()
        abs_shap_z = np.mean(np.abs(shap_z), axis=0)
        scale = float(np.sum(abs_shap_z))

        gene_importance = {}
        for i, gene in enumerate(genes):
            gene_importance[gene] = float(np.mean(np.abs(W_gene_hidden[:, i])) * scale)

        imp_s = pd.Series(gene_importance).reindex(genes)
        all_importances.append(imp_s)
        topk_lists.append(imp_s.sort_values(ascending=False).head(int(top_k)).index.tolist())

        if run_i == int(n_runs) - 1:
            last_shap_z = shap_z
            last_z_test = z_test_np
            abs_latent = np.mean(np.abs(shap_z), axis=0)
            last_latent_df = (
                pd.DataFrame({"LatentDim": np.arange(len(abs_latent)), "MeanAbsSHAP": abs_latent})
                .sort_values("MeanAbsSHAP", ascending=False)
                .reset_index(drop=True)
            )

        prog.progress((run_i + 1) / int(n_runs))

    status.empty()
    prog.empty()

    metrics_df = pd.DataFrame(metrics_runs)
    summary_df = metrics_df[["AUC", "Accuracy", "Precision", "Recall"]].agg(["mean", "std"]).T.reset_index()
    summary_df.columns = ["Metric", "Mean", "Std"]

    imp_mat = pd.concat(all_importances, axis=1)
    imp_mat.columns = [f"run_{i}" for i in range(int(n_runs))]

    mean_imp = imp_mat.mean(axis=1)
    std_imp = imp_mat.std(axis=1)
    cv_imp = std_imp / (mean_imp.abs() + 1e-12)

    from collections import Counter
    freq_counter = Counter([g for lst in topk_lists for g in lst])
    freq = pd.Series({g: freq_counter.get(g, 0) / float(n_runs) for g in genes})
    freq_col = f"Top{int(top_k)}_Freq"

    stability_df = (
        pd.DataFrame(
            {
                "Gene": genes,
                "MeanImportance": mean_imp.values,
                "StdImportance": std_imp.values,
                "CV": cv_imp.values,
                freq_col: freq.values,
            }
        )
        .sort_values([freq_col, "MeanImportance"], ascending=[False, False])
        .reset_index(drop=True)
    )

    top20_genes = stability_df.sort_values("MeanImportance", ascending=False)["Gene"].head(20).tolist()

    st.session_state["cache_rna"] = rna
    st.session_state["cache_labels"] = labels
    st.session_state["cache_top20_genes"] = top20_genes

    st.session_state["cache_metrics_df"] = metrics_df
    st.session_state["cache_summary_df"] = summary_df
    st.session_state["cache_stability_df"] = stability_df
    st.session_state["cache_latent_df"] = last_latent_df

    st.session_state["cache_last_shap_z"] = last_shap_z
    st.session_state["cache_last_z_test"] = last_z_test

    st.session_state["cache_csv_metrics_all"] = to_csv_bytes(metrics_df)
    st.session_state["cache_csv_summary"] = to_csv_bytes(summary_df)
    st.session_state["cache_csv_stability"] = to_csv_bytes(stability_df)
    st.session_state["cache_csv_latent"] = to_csv_bytes(last_latent_df) if last_latent_df is not None else None

    for k in [
        "cache_enrich_go_kegg",
        "cache_de_df",
        "cache_de_groups",
        "cache_cluster_df",
        "cache_cluster_X_scaled",
        "cache_surv_aligned",
        "cache_cox_cluster_summary",
    ]:
        st.session_state.pop(k, None)

    st.success("âœ… ä¸»æµç¨‹è¿è¡Œå®Œæˆï¼šå·²ç¼“å­˜ç»“æœï¼ˆä¸‹è½½/åˆ‡æ¢æ¨¡å—ä¸ä¼šä¸¢å¤±ï¼‰ã€‚")


# =====================================================
# Module â‘ 
# =====================================================
if module.startswith("â‘ "):
    st.subheader("â‘  è®­ç»ƒ / SHAP / ç¨³å®šæ€§ï¼ˆä¸»æµç¨‹å›æ˜¾ï¼‰")

    if "cache_stability_df" not in st.session_state:
        st.info("è¯·å…ˆä¸Šä¼ æ•°æ®å¹¶ç‚¹å‡»ã€ŒğŸš€ è¿è¡Œä¸»æµç¨‹ã€ã€‚")
    else:
        metrics_df = st.session_state["cache_metrics_df"]
        summary_df = st.session_state["cache_summary_df"]
        stability_df = st.session_state["cache_stability_df"]
        latent_df = st.session_state.get("cache_latent_df", None)
        top20 = st.session_state["cache_top20_genes"]

        st.markdown("### ğŸ“Š æ¨¡å‹æ€§èƒ½ï¼ˆæ¯æ¬¡ runï¼‰")
        st.dataframe(metrics_df, use_container_width=True)

        st.markdown("### ğŸ“Š æ¨¡å‹æ€§èƒ½æ±‡æ€»ï¼ˆå‡å€¼Â±æ ‡å‡†å·®ï¼‰")
        st.dataframe(summary_df, use_container_width=True)

        st.markdown("### ğŸ“Œ ç”Ÿç‰©æ ‡å¿—ç‰©ç¨³å®šæ€§ï¼ˆFrequency / CVï¼‰")
        st.dataframe(stability_df.head(50), use_container_width=True)

        st.markdown("### ğŸ§¬ Top 20 æ½œåœ¨ç”Ÿç‰©æ ‡å¿—ç‰©ï¼ˆMeanImportanceï¼‰")
        st.code("\n".join(top20))

        last_shap_z = st.session_state.get("cache_last_shap_z", None)
        last_z_test = st.session_state.get("cache_last_z_test", None)
        if last_shap_z is not None and last_z_test is not None:
            st.markdown("### ğŸ” Latent SHAP Summaryï¼ˆdotï¼‰")
            fig1 = plt.figure()
            shap.summary_plot(last_shap_z, features=last_z_test, show=False)
            st.pyplot(fig1)

            st.markdown("### ğŸ“Š Latent SHAP Summaryï¼ˆbarï¼‰")
            fig2 = plt.figure()
            shap.summary_plot(last_shap_z, features=last_z_test, plot_type="bar", show=False)
            st.pyplot(fig2)

        if latent_df is not None:
            st.markdown("### ğŸ“ˆ Top 20 latent ç»´åº¦é‡è¦æ€§ï¼ˆMeanAbsSHAPï¼‰")
            st.dataframe(latent_df.head(20), use_container_width=True)

            fig3 = plt.figure()
            top_lat = latent_df.head(20)
            plt.bar(top_lat["LatentDim"].astype(str), top_lat["MeanAbsSHAP"])
            plt.xticks(rotation=45, ha="right")
            plt.tight_layout()
            st.pyplot(fig3)


# =====================================================
# Module â‘¡
# =====================================================
if module.startswith("â‘¡"):
    st.subheader("â‘¡ ç»“æœä¸‹è½½ä¸å›æ˜¾ï¼ˆdownload ä¸ä¼šæ¸…ç©ºï¼‰")

    if "cache_stability_df" not in st.session_state:
        st.info("æš‚æ— ç¼“å­˜ç»“æœã€‚è¯·å…ˆè¿è¡Œä¸»æµç¨‹ã€‚")
    else:
        c1, c2, c3, c4 = st.columns(4)
        with c1:
            st.download_button(
                "â¬‡ æ¨¡å‹æŒ‡æ ‡ï¼ˆall runsï¼‰",
                st.session_state.get("cache_csv_metrics_all", b""),
                "model_metrics_all_runs.csv",
                mime="text/csv",
            )
        with c2:
            st.download_button(
                "â¬‡ æŒ‡æ ‡æ±‡æ€»ï¼ˆmeanÂ±stdï¼‰",
                st.session_state.get("cache_csv_summary", b""),
                "model_metrics_summary.csv",
                mime="text/csv",
            )
        with c3:
            st.download_button(
                "â¬‡ åŸºå› ç¨³å®šæ€§ï¼ˆMean/CV/Freqï¼‰",
                st.session_state.get("cache_csv_stability", b""),
                "latent_shap_gene_importance_stability.csv",
                mime="text/csv",
            )
        with c4:
            csv_latent = st.session_state.get("cache_csv_latent", None)
            st.download_button(
                "â¬‡ latent MeanAbsSHAP",
                csv_latent if csv_latent is not None else b"",
                "latent_mean_abs_shap.csv",
                mime="text/csv",
                disabled=(csv_latent is None),
            )

        st.markdown("### Top20 åŸºå› åˆ—è¡¨")
        st.code("\n".join(st.session_state["cache_top20_genes"]))


# =====================================================
# Module â‘¢
# =====================================================
if module.startswith("â‘¢"):
    st.subheader("â‘¢ GO / KEGG å¯Œé›†åˆ†æï¼ˆTop20ï¼‰")

    if "cache_top20_genes" not in st.session_state:
        st.info("è¯·å…ˆè¿è¡Œä¸»æµç¨‹ï¼Œç”Ÿæˆ Top20 åŸºå› ã€‚")
    else:
        top_genes = st.session_state["cache_top20_genes"]
        st.markdown("### è¾“å…¥åŸºå› ï¼ˆTop20ï¼‰")
        st.code("\n".join(top_genes))

        org = st.selectbox("ç‰©ç§ï¼ˆEnrichr organismï¼‰", ["Human", "Mouse"], index=0)

        if not GSEAPY_OK:
            st.warning("æœªå®‰è£… gseapyï¼Œæ— æ³•åš GO/KEGGã€‚è¯·å®‰è£…ï¼špip install gseapy")
        else:
            if st.button("ğŸ§ª è¿è¡Œ GO/KEGGï¼ˆEnrichrï¼‰"):
                with st.spinner("å¯Œé›†åˆ†æè¿è¡Œä¸­ï¼ˆéœ€è¦è”ç½‘è®¿é—® Enrichrï¼‰..."):
                    try:
                        res_dict = run_enrichr(top_genes, organism=org)
                        st.session_state["cache_enrich_go_kegg"] = res_dict
                        st.success("å¯Œé›†å®Œæˆ âœ…")
                    except Exception as e:
                        st.error(f"å¯Œé›†å¤±è´¥ï¼š{e}")

            if "cache_enrich_go_kegg" in st.session_state:
                res_dict = st.session_state["cache_enrich_go_kegg"]
                st.markdown("### ç»“æœå±•ç¤ºï¼ˆæ¯ä¸ªåº“é»˜è®¤å–å‰ 20 æ¡ï¼‰")
                for lib, df in res_dict.items():
                    st.markdown(f"#### {lib}")
                    st.dataframe(df.head(20), use_container_width=True)
                    st.download_button(
                        f"â¬‡ ä¸‹è½½ {lib}",
                        df.to_csv(index=False).encode("utf-8"),
                        f"enrichr_{lib}.csv",
                        mime="text/csv",
                    )

                st.caption("æç¤ºï¼šå¦‚æœéƒ¨ç½²ç¯å¢ƒæ— æ³•è®¿é—®å¤–ç½‘ï¼ŒEnrichr ä¼šå¤±è´¥ã€‚")


# =====================================================
# Module â‘£
# =====================================================
if module.startswith("â‘£"):
    st.subheader("â‘£ å·®å¼‚åˆ†æï¼ˆlabels åˆ†ç»„ï¼ŒTop20ï¼‰")

    if "cache_rna" not in st.session_state or "cache_labels" not in st.session_state:
        st.info("è¯·å…ˆè¿è¡Œä¸»æµç¨‹ã€‚")
    else:
        rna = st.session_state["cache_rna"]
        labels = st.session_state["cache_labels"]
        top_genes = st.session_state["cache_top20_genes"]

        with st.expander("ğŸ” è°ƒè¯•ä¿¡æ¯ï¼ˆlabels çœŸå®åˆ—å reprï¼‰", expanded=False):
            st.write([repr(c) for c in labels.columns])
            st.dataframe(labels.head(10), use_container_width=True)

        if not SCIPY_OK:
            st.warning("æœªå®‰è£… scipyï¼Œæ— æ³•åšå·®å¼‚åˆ†æï¼špip install scipy")
        if not STATSMODELS_OK:
            st.warning("æœªå®‰è£… statsmodelsï¼Œæ— æ³•åš FDRï¼špip install statsmodels")

        if SCIPY_OK and STATSMODELS_OK:
            if st.button("ğŸ§¬ è¿è¡Œ Top20 å·®å¼‚åˆ†æï¼ˆt-test + FDRï¼‰"):
                with st.spinner("å·®å¼‚åˆ†æè®¡ç®—ä¸­..."):
                    try:
                        de_df, groups = compute_de_top_genes(rna, labels, top_genes)
                        st.session_state["cache_de_df"] = de_df
                        st.session_state["cache_de_groups"] = groups
                        st.success("å·®å¼‚åˆ†æå®Œæˆ âœ…")
                    except Exception as e:
                        st.error(f"å·®å¼‚åˆ†æå¤±è´¥ï¼š{e}")

        if "cache_de_df" in st.session_state:
            de_df = st.session_state["cache_de_df"]
            g0, g1 = st.session_state.get("cache_de_groups", ("Group0", "Group1"))

            st.markdown(f"### å·®å¼‚ç»“æœï¼ˆ{g0} vs {g1}ï¼‰")
            st.dataframe(de_df, use_container_width=True)

            st.download_button(
                "â¬‡ ä¸‹è½½å·®å¼‚åˆ†æç»“æœï¼ˆTop20ï¼‰",
                de_df.to_csv(index=False).encode("utf-8"),
                "top20_differential_expression.csv",
                mime="text/csv",
            )

            st.markdown("### ç«å±±å›¾ï¼ˆTop20ï¼‰")
            figv = plt.figure()
            x = de_df["log2FC"].values
            yv = -np.log10(de_df["p_value"].values + 1e-300)
            plt.scatter(x, yv)
            for _, row in de_df.iterrows():
                plt.text(row["log2FC"], -np.log10(row["p_value"] + 1e-300), row["Gene"], fontsize=8)
            plt.xlabel("log2FC")
            plt.ylabel("-log10(p)")
            plt.title("Volcano plot (Top20)")
            plt.tight_layout()
            st.pyplot(figv)

            st.markdown("### ç®±çº¿å›¾ï¼ˆé€‰æ‹©ä¸€ä¸ªåŸºå› ï¼‰")
            gene_pick = st.selectbox("é€‰æ‹©åŸºå› ", de_df["Gene"].tolist(), index=0)

            # group labelsï¼ˆç”¨ normalize ç¡®ä¿ Sample/Label æ­£ç¡®ï¼‰
            lab = labels.copy()
            if "Sample" not in clean_columns(lab).columns or "Label" not in clean_columns(lab).columns:
                lab = normalize_labels_df(lab)
            else:
                lab = clean_columns(lab)
                lab["Sample"] = lab["Sample"].astype(str).str.strip()
            lab2 = lab.set_index("Sample").reindex(rna.columns)

            groups_u = pd.Series(lab2["Label"].values).unique().tolist()
            s0 = lab2[lab2["Label"] == groups_u[0]].index.tolist()
            s1 = lab2[lab2["Label"] == groups_u[1]].index.tolist()

            x0 = rna.loc[gene_pick].reindex(s0).astype(float).values
            x1 = rna.loc[gene_pick].reindex(s1).astype(float).values

            figb = plt.figure()
            plt.boxplot([x0, x1], labels=[str(groups_u[0]), str(groups_u[1])])
            plt.title(f"{gene_pick} expression by Label")
            plt.ylabel("Expression")
            plt.tight_layout()
            st.pyplot(figb)


# =====================================================
# Module â‘¤
# =====================================================
if module.startswith("â‘¤"):
    st.subheader("â‘¤ èšç±»ï¼ˆTop20ï¼‰ + ç”Ÿå­˜åˆ†æï¼ˆKM / Coxï¼‰")

    if "cache_rna" not in st.session_state or "cache_top20_genes" not in st.session_state:
        st.info("è¯·å…ˆè¿è¡Œä¸»æµç¨‹ã€‚")
    else:
        rna = st.session_state["cache_rna"]
        top_genes = st.session_state["cache_top20_genes"]

        st.markdown("### èšç±»è¾“å…¥åŸºå› ï¼ˆTop20ï¼‰")
        st.code("\n".join(top_genes))

        if st.button("ğŸ§© è¿è¡Œèšç±»ï¼ˆTop20 åŸºå› è¡¨è¾¾ï¼‰"):
            with st.spinner("èšç±»ä¸­..."):
                try:
                    cluster_df, X_scaled_df = cluster_samples_by_top_genes(
                        rna=rna,
                        top_genes=top_genes,
                        n_clusters=int(cluster_k),
                        seed=int(seed_base),
                    )
                    st.session_state["cache_cluster_df"] = cluster_df
                    st.session_state["cache_cluster_X_scaled"] = X_scaled_df
                    st.success("èšç±»å®Œæˆ âœ…")
                except Exception as e:
                    st.error(f"èšç±»å¤±è´¥ï¼š{e}")

        if "cache_cluster_df" in st.session_state:
            cluster_df = st.session_state["cache_cluster_df"]
            X_scaled_df = st.session_state.get("cache_cluster_X_scaled", None)

            st.markdown("### èšç±»ç»“æœï¼ˆSample â†’ Clusterï¼‰")
            st.dataframe(cluster_df.head(100), use_container_width=True)

            st.download_button(
                "â¬‡ ä¸‹è½½èšç±»ç»“æœ",
                cluster_df.to_csv(index=False).encode("utf-8"),
                "top20_cluster_labels.csv",
                mime="text/csv",
            )

            if X_scaled_df is not None:
                st.markdown("### çƒ­å›¾ï¼ˆz-scoreï¼Œæ ·æœ¬æŒ‰ Cluster æ’åºï¼‰")
                df_plot = X_scaled_df.copy()
                df_plot["Cluster"] = cluster_df.set_index("Sample").loc[df_plot.index]["Cluster"].values
                df_plot = df_plot.sort_values("Cluster")
                mat = df_plot.drop(columns=["Cluster"]).values

                fig_h = plt.figure(figsize=(10, 5))
                plt.imshow(mat, aspect="auto")
                plt.colorbar(label="z-score")
                plt.yticks([])
                plt.xticks(range(df_plot.shape[1] - 1), df_plot.drop(columns=["Cluster"]).columns, rotation=90, fontsize=7)
                plt.title("Top20 genes (z-score) sorted by Cluster")
                plt.tight_layout()
                st.pyplot(fig_h)

            st.markdown("## ç”Ÿå­˜åˆ†æï¼ˆç”¨ Cluster åˆ†ç»„ï¼‰")
            if surv_file is None:
                st.info("æœªä¸Šä¼ ç”Ÿå­˜æ•°æ®ï¼ˆSample, Time, Eventï¼‰ï¼Œä»…å±•ç¤ºèšç±»ç»“æœã€‚")
            else:
                if not LIFELINES_OK:
                    st.warning("æœªå®‰è£… lifelinesï¼špip install lifelines")
                else:
                    surv = clean_columns(read_csv_cached(surv_file))
                    if "Sample" not in surv.columns:
                        # å°è¯•è‡ªåŠ¨è¯†åˆ« Sample
                        surv_cols = {_norm(c): c for c in surv.columns}
                        for a in ["sample", "sampleid", "id", "subject", "patient"]:
                            if _norm(a) in surv_cols:
                                surv = surv.rename(columns={surv_cols[_norm(a)]: "Sample"})
                                break

                    if "Sample" not in surv.columns or "Time" not in surv.columns or "Event" not in surv.columns:
                        st.error("ç”Ÿå­˜æ•°æ®å¿…é¡»åŒ…å«åˆ—ï¼šSample, Time, Eventï¼ˆSample å¯è‡ªåŠ¨è¯†åˆ«ï¼›Time/Event éœ€åŒåï¼‰")
                    else:
                        surv["Sample"] = surv["Sample"].astype(str).str.strip()
                        surv = surv.set_index("Sample")

                        cl = cluster_df.copy()
                        cl["Sample"] = cl["Sample"].astype(str).str.strip()
                        cl = cl.set_index("Sample")

                        common = sorted(list(set(cl.index).intersection(set(surv.index))))
                        if len(common) < 10:
                            st.error("ç”Ÿå­˜æ•°æ®ä¸èšç±»æ ·æœ¬äº¤é›†å¤ªå°‘ï¼ˆ<10ï¼‰ï¼Œæ— æ³•ç”Ÿå­˜åˆ†æã€‚")
                        else:
                            surv_aligned = surv.loc[common].copy()
                            surv_aligned["Cluster"] = cl.loc[common]["Cluster"].astype(int).values

                            surv_aligned["Time"] = pd.to_numeric(surv_aligned["Time"], errors="coerce")
                            surv_aligned["Event"] = pd.to_numeric(surv_aligned["Event"], errors="coerce")
                            surv_aligned = surv_aligned.dropna(subset=["Time", "Event", "Cluster"])

                            st.markdown("### å¯¹é½åçš„ç”Ÿå­˜æ•°æ®ï¼ˆå« Clusterï¼‰")
                            st.dataframe(surv_aligned.reset_index().head(50), use_container_width=True)

                            fig_km, p_lr = km_plot_by_group(surv_aligned, group_col="Cluster")
                            st.pyplot(fig_km)
                            if p_lr is not None:
                                st.write({"Log-rank p-value (2 groups)": p_lr})

                            st.markdown("### Coxï¼ˆCluster ä½œä¸ºåå˜é‡ï¼‰")
                            df_cox = surv_aligned[["Time", "Event", "Cluster"]].copy()
                            df_cox = df_cox.reset_index(drop=True)
                            df_cox = pd.get_dummies(df_cox, columns=["Cluster"], drop_first=True)

                            df_train, df_test = train_test_split(df_cox, test_size=float(test_size), random_state=int(seed_base))

                            cph = CoxPHFitter(penalizer=float(cox_penalizer))
                            cph.fit(df_train, duration_col="Time", event_col="Event")

                            risk = cph.predict_partial_hazard(df_test)
                            c_index = concordance_index(df_test["Time"], -risk.values, df_test["Event"])
                            st.write({"C-index": float(c_index)})

                            cox_sum = cph.summary.reset_index()
                            st.dataframe(cox_sum, use_container_width=True)

                            st.download_button(
                                "â¬‡ ä¸‹è½½ Cox summaryï¼ˆClusterï¼‰",
                                cox_sum.to_csv(index=False).encode("utf-8"),
                                "cox_cluster_summary.csv",
                                mime="text/csv",
                            )

                            out_risk = pd.DataFrame(
                                {
                                    "Time": df_test["Time"].values,
                                    "Event": df_test["Event"].values,
                                    "RiskScore": risk.values.flatten(),
                                }
                            )
                            st.download_button(
                                "â¬‡ ä¸‹è½½ Cox æµ‹è¯•é›†é£é™©åˆ†æ•°",
                                out_risk.to_csv(index=False).encode("utf-8"),
                                "cox_cluster_test_risk_scores.csv",
                                mime="text/csv",
                            )


# =====================================================
# Footer
# =====================================================
st.divider()
st.caption(
    "ä¾èµ–æç¤ºï¼šåŸºç¡€åŠŸèƒ½éœ€ streamlit/pandas/numpy/torch/scikit-learn/shap/matplotlibï¼›"
    "å·®å¼‚åˆ†æéœ€ scipy + statsmodelsï¼›"
    "GO/KEGGï¼ˆEnrichrï¼‰éœ€ gseapy ä¸”éœ€è¦ç½‘ç»œï¼›"
    "ç”Ÿå­˜åˆ†æéœ€ lifelinesã€‚"
)
